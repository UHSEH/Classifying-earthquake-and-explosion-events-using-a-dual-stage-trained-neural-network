import os
import random
import numpy as np
import pandas as pd
import h5py
from obspy import UTCDateTime
from obspy.signal.invsim import cosine_taper
from obspy.core.trace import Trace, Stats

# -------------------------- 1. åŸºç¡€é…ç½®ï¼ˆä¸å˜ï¼‰ --------------------------
METADATA_PATH = "/home/he/PycharmProjects/PythonProject/dataset/PNW-ML/comcat_metadata.csv"
WAVEFORM_HDF5_PATH = "/home/he/PycharmProjects/PythonProject/dataset/PNW-ML/comcat_waveforms.hdf5"
OUTPUT_ROOT = "/home/he/PycharmProjects/PythonProject/dataset/processed_comcat"
os.makedirs(OUTPUT_ROOT, exist_ok=True)

# å…³é”®å‚æ•°
TRAIN_TEST_SPLIT = 0.9
VAL_SPLIT_FROM_TRAIN = 0.2
AUGMENT_TIMES = 5  # çˆ†ç‚¸äº‹ä»¶å›ºå®šå¢å¼º5å€
SAMPLE_RATE = 100
WAVEFORM_LENGTH = 60
WAVEFORM_POINTS = WAVEFORM_LENGTH * SAMPLE_RATE + 1  # 6001
HIGHPASS_FREQ = 2
BATCH_SIZE = 800
RANDOM_SEED = 42  # å›ºå®šç§å­ç¡®ä¿ç»“æœå¯å¤ç°
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)


# -------------------------- 2. åŠ è½½å…ƒæ•°æ®ï¼ˆä¸è¿›è¡Œå¹³è¡¡ï¼Œåªç­›é€‰æœ‰æ•ˆæ•°æ®ï¼‰ --------------------------
def load_metadata(metadata_path):
    metadata = pd.read_csv(metadata_path)

    required_cols = [
        "event_id", "source_type", "preferred_source_magnitude",
        "source_origin_time", "trace_start_time", "trace_P_arrival_sample",
        "trace_sampling_rate_hz", "trace_name"
    ]
    missing_cols = [col for col in required_cols if col not in metadata.columns]
    if missing_cols:
        raise ValueError(f"ç¼ºå°‘å…³é”®åˆ—ï¼š{missing_cols}")

    # è§£æ trace_name
    parsed_buckets = []
    parsed_tindexes = []
    invalid_count = 0
    for _, row in metadata.iterrows():
        trace_name = str(row["trace_name"])
        try:
            bucket_part = trace_name.split("$")[0]
            tindex_str = trace_name.split("$")[1].split(",")[0]
            parsed_buckets.append(bucket_part)
            parsed_tindexes.append(int(tindex_str))
        except:
            invalid_count += 1
            parsed_buckets.append(None)
            parsed_tindexes.append(None)

    metadata["bucket"] = parsed_buckets
    metadata["tindex"] = parsed_tindexes
    metadata = metadata[metadata["bucket"].notna() & metadata["tindex"].notna()].copy()
    print(f"âœ… trace_name è§£æï¼šæˆåŠŸ{len(metadata)}ä¸ªï¼Œå¤±è´¥{invalid_count}ä¸ª")

    # ç­›é€‰æœ‰æ•ˆäº‹ä»¶
    metadata = metadata[metadata["source_type"].isin(["earthquake", "explosion"])].copy()
    metadata["mag"] = pd.to_numeric(metadata["preferred_source_magnitude"], errors="coerce")
    metadata = metadata[(metadata["mag"].notna()) & (metadata["mag"] >= 0) & (metadata["mag"] <= 10)].copy()
    metadata = metadata[
        (metadata["trace_P_arrival_sample"] >= 0) &
        (metadata["trace_P_arrival_sample"] < metadata["trace_sampling_rate_hz"] * 150)
        ].copy()

    # è®¡ç®—Pæ³¢æ—¶é—´
    p_arrival_times = []
    for _, row in metadata.iterrows():
        try:
            start_utc = UTCDateTime(row["trace_start_time"])
            p_arrival = start_utc + row["trace_P_arrival_sample"] / row["trace_sampling_rate_hz"]
            p_arrival_times.append(p_arrival.isoformat())
        except:
            p_arrival_times.append(None)
    metadata["p_arrival_time"] = p_arrival_times
    metadata = metadata[metadata["p_arrival_time"].notna()].copy()

    # é‡å‘½ååˆ—
    metadata = metadata.rename(columns={"source_type": "event_type", "source_origin_time": "origin_time"})

    # æ·»åŠ éœ‡çº§åˆ†ç®±
    metadata["mag_bin"] = np.select(
        [(metadata["mag"] >= 0) & (metadata["mag"] < 1),
         (metadata["mag"] >= 1) & (metadata["mag"] < 2),
         (metadata["mag"] >= 2) & (metadata["mag"] < 3),
         (metadata["mag"] >= 3) & (metadata["mag"] <= 10)],
        ["0-1", "1-2", "2-3", "3-10"], default="other"
    )

    # ç»Ÿè®¡åŸå§‹æ•°æ®
    explosion_traces = metadata[metadata["event_type"] == "explosion"]
    earthquake_traces = metadata[metadata["event_type"] == "earthquake"]

    print(f"\n åŸå§‹æ•°æ®ç»Ÿè®¡ï¼š")
    print(f"  - åœ°éœ‡traceï¼š{len(earthquake_traces)}ä¸ª")
    print(f"  - çˆ†ç‚¸traceï¼š{len(explosion_traces)}ä¸ª")
    print(f"  - åœ°éœ‡äº‹ä»¶ï¼š{earthquake_traces['event_id'].nunique()}ä¸ª")
    print(f"  - çˆ†ç‚¸äº‹ä»¶ï¼š{explosion_traces['event_id'].nunique()}ä¸ª")

    # ç»Ÿè®¡éœ‡çº§åˆ†ç®±
    print(f"\n éœ‡çº§åˆ†ç®±ç»Ÿè®¡ï¼š")
    for event_type in ["earthquake", "explosion"]:
        type_traces = metadata[metadata["event_type"] == event_type]
        print(f"  - {event_type}:")
        for mag_bin in ["0-1", "1-2", "2-3", "3-10"]:
            bin_count = len(type_traces[type_traces["mag_bin"] == mag_bin])
            if bin_count > 0:
                print(f"    {mag_bin}: {bin_count}ä¸ªtrace")

    return metadata


# -------------------------- 3. äº‹ä»¶çº§æ—¶é—´åˆ†å‰²ï¼ˆä¿æŒéœ‡çº§åˆ†ç®±å¹³è¡¡ï¼‰ --------------------------
def split_train_test_val(metadata_df):
    # ç”Ÿæˆäº‹ä»¶çº§æ•°æ®
    event_level_df = metadata_df.groupby("event_id").agg({
        "origin_time": "first",
        "event_type": "first",
        "mag": "first",
        "mag_bin": "first",
        "trace_name": "count"
    }).rename(columns={"trace_name": "trace_count"}).reset_index()

    # æŒ‰äº‹ä»¶å‘ç”Ÿæ—¶é—´æ’åº
    event_level_df = event_level_df.sort_values("origin_time").reset_index(drop=True)
    total_events = len(event_level_df)
    total_traces = len(metadata_df)

    print(f"\n äº‹ä»¶çº§ç»Ÿè®¡ï¼ˆåˆ†å‰²å‰ï¼‰ï¼š")
    print(f"  - æ€»äº‹ä»¶æ•°ï¼š{total_events}ä¸ª")
    print(f"  - æ€»traceæ•°ï¼š{total_traces}ä¸ª")
    print(f"  - å¹³å‡æ¯ä¸ªäº‹ä»¶traceæ•°ï¼š{total_traces / total_events:.2f}ä¸ª")

    # åˆ†å‰²Train/Testäº‹ä»¶ID
    train_event_num = int(total_events * TRAIN_TEST_SPLIT)
    train_event_ids = set(event_level_df.iloc[:train_event_num]["event_id"].tolist())
    test_event_ids = set(event_level_df.iloc[train_event_num:]["event_id"].tolist())

    # ä»Trainäº‹ä»¶ä¸­åˆ†å‰²Valäº‹ä»¶ID
    train_events_subset = event_level_df[event_level_df["event_id"].isin(train_event_ids)]
    val_event_num = int(len(train_events_subset) * VAL_SPLIT_FROM_TRAIN)
    val_event_ids = set(train_events_subset.sample(val_event_num, random_state=RANDOM_SEED)["event_id"].tolist())
    train_event_ids = train_event_ids - val_event_ids

    # æ ¡éªŒäº‹ä»¶IDæ— é‡å 
    overlap_train_val = train_event_ids & val_event_ids
    overlap_train_test = train_event_ids & test_event_ids
    overlap_val_test = val_event_ids & test_event_ids
    assert len(overlap_train_val) == 0, f"âŒ Trainä¸Valå­˜åœ¨é‡å äº‹ä»¶IDï¼š{list(overlap_train_val)[:5]}..."
    assert len(overlap_train_test) == 0, f"âŒ Trainä¸Testå­˜åœ¨é‡å äº‹ä»¶IDï¼š{list(overlap_train_test)[:5]}..."
    assert len(overlap_val_test) == 0, f"âŒ Valä¸Testå­˜åœ¨é‡å äº‹ä»¶IDï¼š{list(overlap_val_test)[:5]}..."
    print(f"âœ… æ‰€æœ‰å­é›†äº‹ä»¶IDæ— é‡å ï¼Œæ— æ•°æ®æ³„éœ²é£é™©ï¼")

    # æŒ‰äº‹ä»¶IDåˆ†é…å¯¹åº”çš„æ‰€æœ‰traceåˆ°å­é›†
    train_df = metadata_df[metadata_df["event_id"].isin(train_event_ids)].copy().reset_index(drop=True)
    val_df = metadata_df[metadata_df["event_id"].isin(val_event_ids)].copy().reset_index(drop=True)
    test_df = metadata_df[metadata_df["event_id"].isin(test_event_ids)].copy().reset_index(drop=True)

    # ç»Ÿè®¡å„å­é›†çš„äº‹ä»¶å’Œtraceåˆ†å¸ƒ
    def count_subset_stats(df, subset_name):
        total_event = df["event_id"].nunique()
        eq_event = df[df["event_type"] == "earthquake"]["event_id"].nunique()
        ex_event = df[df["event_type"] == "explosion"]["event_id"].nunique()
        total_trace = len(df)
        eq_trace = len(df[df["event_type"] == "earthquake"])
        ex_trace = len(df[df["event_type"] == "explosion"])
        event_ratio = (total_event / total_events) * 100
        trace_ratio = (total_trace / total_traces) * 100
        return {
            "subset": subset_name,
            "total_event": total_event, "eq_event": eq_event, "ex_event": ex_event, "event_ratio": event_ratio,
            "total_trace": total_trace, "eq_trace": eq_trace, "ex_trace": ex_trace, "trace_ratio": trace_ratio
        }

    # ç”Ÿæˆç»Ÿè®¡ç»“æœ
    train_stats = count_subset_stats(train_df, "Train")
    val_stats = count_subset_stats(val_df, "Val")
    test_stats = count_subset_stats(test_df, "Test")

    # æ‰“å°åˆ†å‰²ç»“æœ
    print(f"\n äº‹ä»¶çº§æ—¶é—´åˆ†å‰²æœ€ç»ˆç»“æœï¼š")
    print("  " + "-" * 120)
    print(f"  {'å­é›†':<8} {'äº‹ä»¶æ•°(æ€»/éœ‡/çˆ†)':<20} {'äº‹ä»¶å æ¯”(%)':<12} {'traceæ•°(æ€»/éœ‡/çˆ†)':<20} {'traceå æ¯”(%)':<12}")
    print("  " + "-" * 120)
    for stats in [train_stats, val_stats, test_stats]:
        event_str = f"{stats['total_event']}/{stats['eq_event']}/{stats['ex_event']}"
        trace_str = f"{stats['total_trace']}/{stats['eq_trace']}/{stats['ex_trace']}"
        print(
            f"  {stats['subset']:<8} {event_str:<20} {stats['event_ratio']:<12.1f} {trace_str:<20} {stats['trace_ratio']:<12.1f}")
    print("  " + "-" * 120)
    print(
        f"  {'æ€»è®¡':<8} {total_events}/{event_level_df[event_level_df['event_type'] == 'earthquake'].shape[0]}/{event_level_df[event_level_df['event_type'] == 'explosion'].shape[0]:<12} 100.0{'':<10} {total_traces}/{len(metadata_df[metadata_df['event_type'] == 'earthquake'])}/{len(metadata_df[metadata_df['event_type'] == 'explosion']):<12} 100.0")

    return train_df, val_df, test_df


# -------------------------- 4. è¯»å–HDF5æ³¢å½¢æ•°æ®ï¼ˆä¸å˜ï¼‰ --------------------------
def read_waveform_from_hdf5(hdf5_file, bucket_name, tindex, sampling_rate, start_time):
    waveform_traces = []
    required_comps = ["Z", "N", "E"]
    bucket_path = f"data/{bucket_name}"

    if bucket_path not in hdf5_file:
        raise ValueError(f"HDF5æ— bucketï¼š{bucket_path}")

    bucket_dataset = hdf5_file[bucket_path]
    n_events = bucket_dataset.shape[0]
    if tindex < 0 or tindex >= n_events:
        raise ValueError(f"tindex {tindex} è¶…å‡ºèŒƒå›´ï¼ˆ0~{n_events - 1}ï¼‰")

    wave_data = bucket_dataset[tindex]
    if wave_data.shape[0] != 3:
        raise ValueError(f"åˆ†é‡æ•°å¼‚å¸¸ï¼š{wave_data.shape[0]}ï¼ˆéœ€ä¸º3ï¼‰")

    comp_order = hdf5_file["data_format/component_order"][()].decode('utf-8')
    comp_map = {c: i for i, c in enumerate(comp_order)}
    missing = [c for c in required_comps if c not in comp_map]
    if missing:
        raise ValueError(f"ç¼ºå°‘åˆ†é‡ï¼š{missing}ï¼ˆé¡ºåºï¼š{comp_order}ï¼‰")

    start_utc = UTCDateTime(start_time)
    for comp in required_comps:
        comp_data = wave_data[comp_map[comp]].astype(np.float32)
        stats = Stats({
            "npts": len(comp_data),
            "sampling_rate": sampling_rate,
            "starttime": start_utc,
            "channel": comp
        })
        waveform_traces.append(Trace(data=comp_data, header=stats))

    return waveform_traces


# -------------------------- 5. æ³¢å½¢é¢„å¤„ç†ï¼ˆä¸å˜ï¼‰ --------------------------
def preprocess_waveform(waveform_traces, p_arrival_time, random_seed=None):
    processed_comps = []

    try:
        p_utc = UTCDateTime(p_arrival_time)
        p_ts = p_utc.timestamp
    except:
        print("âš ï¸ Pæ³¢æ—¶é—´æ— æ•ˆï¼Œé›¶å¡«å……")
        return np.zeros((WAVEFORM_POINTS, 3), dtype=np.float32)

    try:
        wave_ts = waveform_traces[0].stats.starttime.timestamp
        time_before_p = p_ts - wave_ts
    except:
        time_before_p = 0

    if random_seed is not None:
        random.seed(random_seed)
    if time_before_p >= 20:
        offset = random.uniform(5, 20)
    elif 5 <= time_before_p < 20:
        offset = random.uniform(5, time_before_p)
    else:
        offset = 0

    for trace in waveform_traces:
        trace = trace.copy()
        if trace.stats.sampling_rate != SAMPLE_RATE:
            trace.resample(SAMPLE_RATE)
        trace.detrend("constant")
        trace.detrend("linear")
        trace.data *= cosine_taper(trace.stats.npts, 0.05)
        trace.filter("highpass", freq=HIGHPASS_FREQ, corners=4, zerophase=True)

        try:
            cut_start = trace.stats.starttime + offset
            cut_end = cut_start + WAVEFORM_LENGTH
            cut_data = trace.slice(cut_start, cut_end).data
            if len(cut_data) < WAVEFORM_POINTS:
                cut_data = np.pad(cut_data, (0, WAVEFORM_POINTS - len(cut_data)), "constant")
            else:
                cut_data = cut_data[:WAVEFORM_POINTS]
        except:
            cut_data = np.zeros(WAVEFORM_POINTS, dtype=np.float32)

        max_amp = np.max(np.abs(cut_data))
        if max_amp != 0:
            cut_data /= max_amp
        processed_comps.append(cut_data)

    return np.stack(processed_comps, axis=-1)


# -------------------------- 6. äº‹ä»¶çº§åˆ«å¹³è¡¡ï¼ˆæŒ‰éœ‡çº§åˆ†ç®±ï¼‰ --------------------------
def balance_events_by_mag_bin(df):
    """
    æŒ‰éœ‡çº§åˆ†ç®±è¿›è¡Œäº‹ä»¶çº§åˆ«å¹³è¡¡
    è¿”å›å¹³è¡¡åçš„äº‹ä»¶IDåˆ—è¡¨
    """
    # æŒ‰äº‹ä»¶åˆ†ç»„ï¼Œæ¯ä¸ªäº‹ä»¶å–ç¬¬ä¸€æ¡è®°å½•
    event_first_traces = df.groupby("event_id").first().reset_index()

    # åˆ†åˆ«ç»Ÿè®¡åœ°éœ‡å’Œçˆ†ç‚¸äº‹ä»¶
    earthquake_events = event_first_traces[event_first_traces["event_type"] == "earthquake"]
    explosion_events = event_first_traces[event_first_traces["event_type"] == "explosion"]

    print(f"\n äº‹ä»¶å¹³è¡¡å‰ç»Ÿè®¡ï¼š")
    print(f"  - åœ°éœ‡äº‹ä»¶ï¼š{len(earthquake_events)}ä¸ª")
    print(f"  - çˆ†ç‚¸äº‹ä»¶ï¼š{len(explosion_events)}ä¸ª")

    # æŒ‰éœ‡çº§åˆ†ç®±å¹³è¡¡
    balanced_earthquake_events = []
    balanced_explosion_events = []

    for mag_bin in ["0-1", "1-2", "2-3", "3-10"]:
        eq_bin_events = earthquake_events[earthquake_events["mag_bin"] == mag_bin]
        ex_bin_events = explosion_events[explosion_events["mag_bin"] == mag_bin]

        min_count = min(len(eq_bin_events), len(ex_bin_events))

        if min_count == 0:
            print(f"âš ï¸  éœ‡çº§åˆ†ç®± {mag_bin} ä¸­æŸä¸€ç±»äº‹ä»¶æ•°é‡ä¸º0ï¼Œè·³è¿‡è¯¥åˆ†ç®±")
            continue

        # ä»æ¯ä¸ªåˆ†ç®±ä¸­éšæœºé€‰æ‹©ç›¸åŒæ•°é‡çš„äº‹ä»¶
        if len(eq_bin_events) > 0:
            balanced_eq = eq_bin_events.sample(min_count, random_state=RANDOM_SEED)
            balanced_earthquake_events.append(balanced_eq)

        if len(ex_bin_events) > 0:
            balanced_ex = ex_bin_events.sample(min_count, random_state=RANDOM_SEED)
            balanced_explosion_events.append(balanced_ex)

        print(f"  - éœ‡çº§åˆ†ç®± {mag_bin}: åœ°éœ‡{len(balanced_eq)}ä¸ªäº‹ä»¶, çˆ†ç‚¸{len(balanced_ex)}ä¸ªäº‹ä»¶")

    # åˆå¹¶å¹³è¡¡åçš„äº‹ä»¶
    if balanced_earthquake_events:
        balanced_earthquake_events = pd.concat(balanced_earthquake_events, ignore_index=True)
    else:
        balanced_earthquake_events = pd.DataFrame(columns=earthquake_events.columns)

    if balanced_explosion_events:
        balanced_explosion_events = pd.concat(balanced_explosion_events, ignore_index=True)
    else:
        balanced_explosion_events = pd.DataFrame(columns=explosion_events.columns)

    balanced_events = pd.concat([balanced_earthquake_events, balanced_explosion_events], ignore_index=True)
    balanced_event_ids = balanced_events["event_id"].tolist()

    print(f"\nâœ… äº‹ä»¶å¹³è¡¡åç»Ÿè®¡ï¼š")
    print(f"  - åœ°éœ‡äº‹ä»¶ï¼š{len(balanced_earthquake_events)}ä¸ª")
    print(f"  - çˆ†ç‚¸äº‹ä»¶ï¼š{len(balanced_explosion_events)}ä¸ª")
    print(f"  - æ€»äº‹ä»¶ï¼š{len(balanced_events)}ä¸ª")

    return balanced_event_ids


# -------------------------- 7. è®­ç»ƒé›†TRACEçº§åˆ«å¹³è¡¡å’Œæ•°æ®å¢å¼ºï¼ˆä»¥å°‘çš„äº‹ä»¶ä¸ºåŸºå‡†ï¼‰ --------------------------
def balance_and_augment_training_traces(df, balanced_event_ids):
    """
    å¯¹è®­ç»ƒé›†è¿›è¡ŒTRACEçº§åˆ«å¹³è¡¡å’Œæ•°æ®å¢å¼º
    çˆ†ç‚¸äº‹ä»¶å›ºå®šå¢å¼º5å€ï¼Œåœ°éœ‡äº‹ä»¶æ ¹æ®éœ‡çº§åˆ†ç®±å¢å¼ºä»¥è¾¾åˆ°å¹³è¡¡
    """
    print(f"\n æ­£åœ¨è¿›è¡Œè®­ç»ƒé›†TRACEçº§åˆ«å¹³è¡¡å’Œæ•°æ®å¢å¼º...")
    print(f" çˆ†ç‚¸äº‹ä»¶å›ºå®šå¢å¼º{AUGMENT_TIMES}å€ï¼Œåœ°éœ‡äº‹ä»¶æ ¹æ®éœ‡çº§åˆ†ç®±è°ƒæ•´å¢å¼ºå€æ•°ä»¥è¾¾åˆ°å¹³è¡¡")

    # ç­›é€‰å¹³è¡¡åçš„äº‹ä»¶å¯¹åº”çš„æ‰€æœ‰trace
    balanced_df = df[df["event_id"].isin(balanced_event_ids)].copy()

    # æŒ‰éœ‡çº§åˆ†ç®±åˆ†åˆ«å¤„ç†
    augmented_traces = []

    for mag_bin in ["0-1", "1-2", "2-3", "3-10"]:
        # è·å–å½“å‰éœ‡çº§åˆ†ç®±çš„æ‰€æœ‰trace
        bin_traces = balanced_df[balanced_df["mag_bin"] == mag_bin]

        if len(bin_traces) == 0:
            print(f"âš ï¸  éœ‡çº§åˆ†ç®± {mag_bin} æ— traceï¼Œè·³è¿‡")
            continue

        # åˆ†ç¦»åœ°éœ‡å’Œçˆ†ç‚¸trace
        eq_traces = bin_traces[bin_traces["event_type"] == "earthquake"]
        ex_traces = bin_traces[bin_traces["event_type"] == "explosion"]

        # çˆ†ç‚¸äº‹ä»¶å›ºå®šå¢å¼º5å€
        ex_augmented_count = len(ex_traces) * AUGMENT_TIMES

        # è®¡ç®—åœ°éœ‡äº‹ä»¶éœ€è¦å¢å¼ºçš„å€æ•°ä»¥è¾¾åˆ°å¹³è¡¡
        if len(eq_traces) > 0:
            eq_augment_factor = max(1, ex_augmented_count // len(eq_traces))
            if ex_augmented_count % len(eq_traces) != 0:
                eq_augment_factor += 1  # å‘ä¸Šå–æ•´ç¡®ä¿è¶³å¤Ÿæ•°é‡
        else:
            eq_augment_factor = 0

        print(f"\n éœ‡çº§åˆ†ç®± {mag_bin}:")
        print(f"  - åŸå§‹åœ°éœ‡trace: {len(eq_traces)}, çˆ†ç‚¸trace: {len(ex_traces)}")
        print(f"  - çˆ†ç‚¸å¢å¼º{AUGMENT_TIMES}å€å: {ex_augmented_count}ä¸ª")
        print(f"  - åœ°éœ‡éœ€è¦å¢å¼º{eq_augment_factor}å€ä»¥è¾¾åˆ°å¹³è¡¡")

        # å¯¹çˆ†ç‚¸traceè¿›è¡Œå¢å¼ºï¼ˆå›ºå®š5å€ï¼‰
        for _, trace_row in ex_traces.iterrows():
            for aug_idx in range(AUGMENT_TIMES):
                augmented_trace = trace_row.copy()
                augmented_trace["augment_seed"] = hash(f"{trace_row.name}_ex_{aug_idx}") % 1000000
                augmented_traces.append(augmented_trace)

        # å¯¹åœ°éœ‡traceè¿›è¡Œå¢å¼ºï¼ˆæ ¹æ®éœ‡çº§åˆ†ç®±è®¡ç®—å€æ•°ï¼‰
        for _, trace_row in eq_traces.iterrows():
            for aug_idx in range(eq_augment_factor):
                augmented_trace = trace_row.copy()
                augmented_trace["augment_seed"] = hash(f"{trace_row.name}_eq_{aug_idx}") % 1000000
                augmented_traces.append(augmented_trace)

    # è½¬æ¢ä¸ºDataFrame
    augmented_df = pd.DataFrame(augmented_traces)

    # å¦‚æœåˆ—åé‡å¤ï¼Œé‡ç½®ç´¢å¼•
    if 'index' in augmented_df.columns:
        augmented_df = augmented_df.reset_index(drop=True)

    # ç»Ÿè®¡å¢å¼ºåçš„åˆ†å¸ƒ
    print(f"\nâœ… è®­ç»ƒé›†TRACEçº§åˆ«å¹³è¡¡å’Œæ•°æ®å¢å¼ºå®Œæˆï¼š")
    print(f"  - å¢å¼ºåæ€»traceæ•°ï¼š{len(augmented_df)}")

    total_eq = len(augmented_df[augmented_df["event_type"] == "earthquake"])
    total_ex = len(augmented_df[augmented_df["event_type"] == "explosion"])
    print(f"  - æ€»åœ°éœ‡trace: {total_eq}, æ€»çˆ†ç‚¸trace: {total_ex}")

    for mag_bin in ["0-1", "1-2", "2-3", "3-10"]:
        bin_traces = augmented_df[augmented_df["mag_bin"] == mag_bin]
        if len(bin_traces) > 0:
            eq_count = len(bin_traces[bin_traces["event_type"] == "earthquake"])
            ex_count = len(bin_traces[bin_traces["event_type"] == "explosion"])
            balance_ratio = min(eq_count, ex_count) / max(eq_count, ex_count) if max(eq_count, ex_count) > 0 else 0
            print(f"  - éœ‡çº§åˆ†ç®± {mag_bin}: åœ°éœ‡{eq_count}, çˆ†ç‚¸{ex_count}, å¹³è¡¡æ¯”: {balance_ratio:.3f}")

    return augmented_df


# -------------------------- 8. HDF5å¢é‡å†™å…¥ --------------------------
def save_dataset_with_hdf5(df, save_dir, is_train=True):
    os.makedirs(save_dir, exist_ok=True)
    augment_times = AUGMENT_TIMES if is_train else 1

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    hdf5_path = os.path.join(save_dir, "dataset.h5")
    metadata_path = os.path.join(save_dir, "metadata.csv")
    event_ids_path = os.path.join(save_dir, "event_ids.npy")

    # å¯¹æ•°æ®é›†è¿›è¡Œå¹³è¡¡
    print(f"\nâš–ï¸  æ­£åœ¨å¹³è¡¡{'è®­ç»ƒé›†' if is_train else 'éªŒè¯/æµ‹è¯•é›†'}...")

    if is_train:
        # è®­ç»ƒé›†ï¼šå…ˆäº‹ä»¶çº§å¹³è¡¡ï¼Œå†TRACEçº§å¹³è¡¡å’Œæ•°æ®å¢å¼º
        balanced_event_ids = balance_events_by_mag_bin(df)
        balanced_df = balance_and_augment_training_traces(df, balanced_event_ids)
    else:
        # éªŒè¯/æµ‹è¯•é›†ï¼šåªè¿›è¡Œäº‹ä»¶çº§å¹³è¡¡ï¼Œä¿ç•™æ‰€æœ‰trace
        balanced_event_ids = balance_events_by_mag_bin(df)
        balanced_df = df[df["event_id"].isin(balanced_event_ids)].copy()

    # ä¿å­˜å¹³è¡¡åçš„å…ƒæ•°æ®
    balanced_df.to_csv(metadata_path, index=False, encoding="utf-8")

    total_events = balanced_df["event_id"].nunique()
    total_traces = len(balanced_df)
    print(f"ğŸ“„ å·²ä¿å­˜å…ƒæ•°æ®ï¼š{metadata_path}ï¼ˆ{total_traces}ä¸ªtraceï¼Œ{total_events}ä¸ªäº‹ä»¶ï¼‰")

    # åˆå§‹åŒ–HDF5æ–‡ä»¶
    with h5py.File(hdf5_path, "w") as hf:
        hf.create_dataset(
            "waveforms",
            shape=(0, WAVEFORM_POINTS, 3),
            dtype=np.float32,
            maxshape=(None, WAVEFORM_POINTS, 3),
            chunks=True
        )
        hf.create_dataset(
            "labels",
            shape=(0,),
            dtype=np.int8,
            maxshape=(None,),
            chunks=True
        )
        # ä¿å­˜å½“å‰å­é›†çš„äº‹ä»¶IDåˆ—è¡¨
        subset_event_ids = np.array(balanced_df["event_id"].unique(), dtype="S")
        hf.create_dataset("subset_event_ids", data=subset_event_ids)

    # æ”¶é›†æ‰€æœ‰äº‹ä»¶ID
    all_event_ids = []

    # åˆ†æ‰¹å¤„ç†å¹¶å¢é‡å†™å…¥
    current_offset = 0

    # å‡†å¤‡è¦å¤„ç†çš„traceåˆ—è¡¨
    trace_groups = [(idx, row) for idx, row in balanced_df.iterrows()]
    total_traces_to_process = len(trace_groups)

    for batch_start in range(0, total_traces_to_process, BATCH_SIZE):
        batch_traces = trace_groups[batch_start:batch_start + BATCH_SIZE]
        if not batch_traces:
            break
        batch_waveforms = []
        batch_labels = []
        batch_event_ids_list = []

        with h5py.File(WAVEFORM_HDF5_PATH, "r") as src_hf:
            for idx, trace_row in batch_traces:
                event_id = trace_row["event_id"]
                label = 0 if trace_row["event_type"] == "earthquake" else 1
                p_arrival = trace_row["p_arrival_time"]

                try:
                    traces = read_waveform_from_hdf5(
                        src_hf,
                        bucket_name=trace_row["bucket"],
                        tindex=trace_row["tindex"],
                        sampling_rate=trace_row["trace_sampling_rate_hz"],
                        start_time=trace_row["trace_start_time"]
                    )
                except Exception as e:
                    print(f"âš ï¸  è·³è¿‡äº‹ä»¶{event_id}ï¼š{e}")
                    continue

                # æ•°æ®å¢å¼º - å¯¹æ¯ä¸ªtraceè¿›è¡Œå¢å¼º
                if is_train and "augment_seed" in trace_row:
                    # è®­ç»ƒé›†ä½¿ç”¨é¢„è®¡ç®—çš„å¢å¼ºç§å­
                    random_seed = trace_row["augment_seed"]
                else:
                    # éªŒè¯/æµ‹è¯•é›†æˆ–æ²¡æœ‰é¢„è®¡ç®—ç§å­çš„æƒ…å†µ
                    random_seed = hash(f"{event_id}_{idx}") % 1000000

                wave = preprocess_waveform(traces, p_arrival, random_seed)
                batch_waveforms.append(wave)
                batch_labels.append(label)
                batch_event_ids_list.append(event_id)

                # æ‰“å°è¿›åº¦
                current_idx = batch_start + len(batch_waveforms)
                if current_idx % 100 == 0 or current_idx >= total_traces_to_process:
                    print(f"ğŸ”„ å·²å¤„ç†traceï¼š{current_idx}/{total_traces_to_process}ï¼ˆ{os.path.basename(save_dir)}ï¼‰")

        # å¢é‡å†™å…¥HDF5
        if batch_waveforms:
            batch_waveforms = np.array(batch_waveforms, dtype=np.float32)
            batch_labels = np.array(batch_labels, dtype=np.int8)
            batch_size = len(batch_waveforms)

            with h5py.File(hdf5_path, "a") as hf:
                hf["waveforms"].resize(current_offset + batch_size, axis=0)
                hf["labels"].resize(current_offset + batch_size, axis=0)
                hf["waveforms"][current_offset:current_offset + batch_size] = batch_waveforms
                hf["labels"][current_offset:current_offset + batch_size] = batch_labels

            current_offset += batch_size
            all_event_ids.extend(batch_event_ids_list)
            del batch_waveforms, batch_labels  # é‡Šæ”¾å†…å­˜

    # ä¿å­˜äº‹ä»¶ID
    np.save(event_ids_path, np.array(all_event_ids, dtype=str))

    # æœ€ç»ˆç»Ÿè®¡å’Œå¹³è¡¡æ€§éªŒè¯
    with h5py.File(hdf5_path, "r") as hf:
        total_waveforms = hf["waveforms"].shape[0]
        labels = hf["labels"][:]

    earthquake_count = np.sum(labels == 0)
    explosion_count = np.sum(labels == 1)

    print(f"\nâœ… {os.path.basename(save_dir)} ä¿å­˜å®Œæˆï¼š")
    print(f"   - äº‹ä»¶æ•°ï¼š{total_events}ä¸ª")
    print(f"   - æ€»æ³¢å½¢æ•°ï¼š{total_waveforms}ä¸ª")
    if is_train:
        print(f"   - çˆ†ç‚¸äº‹ä»¶å›ºå®šå¢å¼ºï¼š{AUGMENT_TIMES}å€")
        print(f"   - åœ°éœ‡äº‹ä»¶åŠ¨æ€å¢å¼ºï¼šæ ¹æ®éœ‡çº§åˆ†ç®±è°ƒæ•´")
    print(f"   - åœ°éœ‡æ³¢å½¢ï¼š{earthquake_count}ä¸ª ({earthquake_count / total_waveforms * 100:.1f}%)")
    print(f"   - çˆ†ç‚¸æ³¢å½¢ï¼š{explosion_count}ä¸ª ({explosion_count / total_waveforms * 100:.1f}%)")

    # æ£€æŸ¥å¹³è¡¡æ€§
    balance_ratio = min(earthquake_count, explosion_count) / max(earthquake_count, explosion_count)
    if is_train and balance_ratio < 0.95:  # è®­ç»ƒé›†è¦æ±‚ä¸¥æ ¼å¹³è¡¡
        print(f"âš ï¸  è­¦å‘Šï¼šè®­ç»ƒé›†ä¸å¹³è¡¡ï¼Œå¹³è¡¡æ¯”ï¼š{balance_ratio:.3f}")
    elif not is_train and balance_ratio < 0.8:  # éªŒè¯/æµ‹è¯•é›†å…è®¸ä¸€å®šä¸å¹³è¡¡
        print(f"âš ï¸  è­¦å‘Šï¼šéªŒè¯/æµ‹è¯•é›†ä¸¥é‡ä¸å¹³è¡¡ï¼Œå¹³è¡¡æ¯”ï¼š{balance_ratio:.3f}")
    else:
        print(f"âœ… æ•°æ®é›†å¹³è¡¡æ€§è‰¯å¥½ï¼Œå¹³è¡¡æ¯”ï¼š{balance_ratio:.3f}")

    print(f"   - HDF5è·¯å¾„ï¼š{hdf5_path}")


# -------------------------- 9. ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    print("=" * 60)
    print(" å¼€å§‹ ComCat æ•°æ®é›†å¤„ç†ï¼ˆéœ‡çº§åˆ†ç®±å¹³è¡¡ï¼‰")
    print("=" * 60)

    try:
        print("\nã€1/3ã€‘ åŠ è½½å…ƒæ•°æ®...")
        metadata_df = load_metadata(METADATA_PATH)

        print("\nã€2/3ã€‘  äº‹ä»¶çº§æ—¶é—´åˆ†å‰²æ•°æ®é›†ï¼ˆæ— æ³„éœ²ï¼‰...")
        train_df, val_df, test_df = split_train_test_val(metadata_df)

        print("\nã€3/3ã€‘ å¤„ç†å¹¶ä¿å­˜æ•°æ®é›†...")
        print("\n  å¤„ç†è®­ç»ƒé›†ï¼ˆäº‹ä»¶å¹³è¡¡ + TRACEçº§åˆ«å¹³è¡¡ + çˆ†ç‚¸5å€å¢å¼º + åœ°éœ‡åŠ¨æ€å¢å¼ºï¼‰...")
        save_dataset_with_hdf5(train_df, os.path.join(OUTPUT_ROOT, "train"), is_train=True)

        print("\n  å¤„ç†éªŒè¯é›†ï¼ˆäº‹ä»¶å¹³è¡¡ï¼Œä¿ç•™æ‰€æœ‰TRACEï¼Œæ— å¢å¼ºï¼‰...")
        save_dataset_with_hdf5(val_df, os.path.join(OUTPUT_ROOT, "val"), is_train=False)

        print("\n  å¤„ç†æµ‹è¯•é›†ï¼ˆäº‹ä»¶å¹³è¡¡ï¼Œä¿ç•™æ‰€æœ‰TRACEï¼Œæ— å¢å¼ºï¼‰...")
        save_dataset_with_hdf5(test_df, os.path.join(OUTPUT_ROOT, "test"), is_train=False)

        print("\n" + "=" * 60)
        print(" æ‰€æœ‰å¤„ç†å®Œæˆï¼")
        print(f" æœ€ç»ˆæ•°æ®å­˜å‚¨è·¯å¾„ï¼š{OUTPUT_ROOT}")
        print(" å·²é€šè¿‡éœ‡çº§åˆ†ç®±å¹³è¡¡ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å¹³è¡¡")
        print("=" * 60)
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥ï¼š{str(e)}")
        import traceback

        traceback.print_exc()
        exit(1)
