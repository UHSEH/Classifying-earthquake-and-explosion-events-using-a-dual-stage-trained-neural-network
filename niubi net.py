import numpy as np
import pandas as pd
import h5py
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix
from scipy.signal import spectrogram
import obspy
from obspy.core.trace import Trace
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import os
import joblib
import psutil
import logging
import gc
import time
from contextlib import contextmanager


@tf.keras.utils.register_keras_serializable()
class QualityAwareAttentionLayer(layers.Layer):
    """è´¨é‡æ„ŸçŸ¥æ³¨æ„åŠ›å±‚ - æ”¯æŒmasking"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.content_dense1 = layers.Dense(64, activation='relu')
        self.content_dense2 = layers.Dense(1, activation='linear')
        self.supports_masking = True  # ğŸ› ï¸ ä¿®å¤ï¼šå£°æ˜æ”¯æŒmasking

    def build(self, input_shape):
        feature_shape, feat_input_shape = input_shape
        # ç»Ÿä¸€è½¬ tupleï¼Œé¿å… tuple + list
        feature_shape = tf.TensorShape(feature_shape)
        self.content_dense1.build(feature_shape)
        # è®¡ç®— 1 ç»´è¾“å‡ºå½¢çŠ¶
        dense1_output_shape = feature_shape[:-1] + (64,)
        self.content_dense2.build(dense1_output_shape)
        super().build(input_shape)

    def call(self, inputs):
        features, feat_inputs = inputs

        # åŸºç¡€å†…å®¹æ³¨æ„åŠ›
        content_attention = self.content_dense1(features)
        content_attention = self.content_dense2(content_attention)  # (batch_size, num_traces, 1)

        # ä»ç‰¹å¾è¾“å…¥ä¸­æå–è´¨é‡åˆ†æ•°ï¼ˆç¬¬10ç»´ï¼‰
        quality_scores = feat_inputs[:, :, -1]  # (batch_size, num_traces)
        quality_scores = tf.expand_dims(quality_scores, axis=-1)  # (batch_size, num_traces, 1)

        # ç”¨è´¨é‡åˆ†æ•°è°ƒæ•´æ³¨æ„åŠ›
        quality_adjustment = tf.where(
            quality_scores >= 0,
            tf.exp(quality_scores * 2),  # æ­£è´¨é‡ï¼šæŒ‡æ•°å¢å¼º
            tf.sigmoid(quality_scores * 10)  # è´Ÿè´¨é‡ï¼šsigmoidæŠ‘åˆ¶åˆ°æ¥è¿‘0
        )

        adjusted_attention = content_attention * quality_adjustment
        adjusted_attention = tf.squeeze(adjusted_attention, axis=-1)  # (batch_size, num_traces)

        # softmaxå½’ä¸€åŒ–
        attention_weights = tf.nn.softmax(adjusted_attention, axis=1)  # (batch_size, num_traces)

        return tf.expand_dims(attention_weights, -1)  # (batch_size, num_traces, 1)

    def compute_mask(self, inputs, mask=None):
        # ğŸ› ï¸ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†mask
        if mask is not None:
            return mask[0]  # è¿”å›ç¬¬ä¸€ä¸ªè¾“å…¥çš„mask
        return None

    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
def quality_function(y_true, y_pred):
    """
    è´¨é‡å‡½æ•°: quality = (2 * y - 1) * (2 * p - 1)
    å€¼åŸŸ [-1, 1]ï¼Œæ­£ç¡®â†’æ­£ï¼Œé”™è¯¯â†’è´Ÿï¼Œç¡®ä¿¡â†’|q|â†’1
    """
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    return (2.0 * y_true - 1.0) * (2.0 * y_pred - 1.0)


@tf.keras.utils.register_keras_serializable()
def bulletproof_trace_loss(y_true, y_pred):
    """ä¿®å¤çš„Traceçº§æŸå¤±å‡½æ•°"""
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)

    # åˆ›å»ºæœ‰æ•ˆæ ·æœ¬æ©ç ï¼ˆæ’é™¤å¡«å……çš„-1ï¼‰
    valid_mask = tf.cast(tf.not_equal(y_true, -1.0), tf.float32)

    # äºŒå…ƒäº¤å‰ç†µ
    bce = - (y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))

    # åªå¯¹æœ‰æ•ˆæ ·æœ¬è®¡ç®—æŸå¤±
    bce = bce * valid_mask
    valid_count = tf.reduce_sum(valid_mask)

    # é˜²æ­¢é™¤é›¶
    loss = tf.cond(valid_count > 0,
                   lambda: tf.reduce_sum(bce) / valid_count,
                   lambda: tf.constant(0.0))

    return tf.maximum(loss, 0.0)


@tf.keras.utils.register_keras_serializable()
def bulletproof_event_loss(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.clip_by_value(y_pred, 1e-3, 1 - 1e-3)
    eps = 1e-7
    loss = - (y_true * tf.math.log(y_pred + eps) +
              (1 - y_true) * tf.math.log(1 - y_pred + eps))
    return tf.maximum(tf.reduce_mean(loss), 0.0)
# ========== æ–°å¢ï¼šä¼˜åŒ–å™¨å°è£… + è´Ÿ loss æ—©æœŸåœè®­ ==========
def build_safe_optimizer(lr):
    return keras.optimizers.Adam(
        learning_rate=lr,
        global_clipnorm=1.0,   # å…¨å±€èŒƒæ•°è£å‰ª
        epsilon=1e-6           # é˜²æ­¢é™¤ä»¥ 0
    )


class KillOnNegativeLoss(Callback):
    def __init__(self, patience=3):
        super().__init__()
        self.patience = patience
        self.count = 0

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            return
        main_loss = logs.get('loss')
        if main_loss is not None and main_loss < 0:
            self.count += 1
            print(f"\n KillOnNegativeLoss: ç¬¬ {self.count} æ¬¡è´Ÿ loss ({main_loss:.4f})")
            if self.count >= self.patience:
                print(" åœæ­¢è®­ç»ƒï¼Œæ£€æŸ¥æŸå¤±å‡½æ•°ï¼")
                self.model.stop_training = True

@tf.keras.utils.register_keras_serializable()
def stable_weighted_binary_crossentropy(class_weights):
    """æ•°å€¼ç¨³å®šçš„åŠ æƒäºŒå…ƒäº¤å‰ç†µ"""
    def loss_fn(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.clip_by_value(y_pred, 1e-5, 1.0 - 1e-5)

        bce = keras.losses.binary_crossentropy(y_true, y_pred)
        weights = tf.where(
            tf.equal(y_true, 1),
            class_weights[1],
            class_weights[0]
        )

        loss = tf.reduce_mean(bce * weights)
        loss = tf.maximum(loss, 0.0)  # ç¡®ä¿ä¸ä¼šå‡ºç°è´Ÿæ•°

        return loss
    return loss_fn


# è®¾ç½®GPUå†…å­˜å¢é•¿ï¼Œé¿å…OOMé”™è¯¯
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"å·²é…ç½®GPUå†…å­˜è‡ªå¢é•¿: {[gpu.name for gpu in gpus]}")
    except RuntimeError as e:
        print(f"GPUé…ç½®é”™è¯¯: {e}")

# ç¡®ä¿Eager Executionå·²å¯ç”¨
if not tf.executing_eagerly():
    tf.compat.v1.enable_eager_execution()
    print("å·²å¯ç”¨Eager Executionæ¨¡å¼")

# -------------------------- é…ç½®å‚æ•° --------------------------
RESULT_OUTPUT_PATH = "/home/he/PycharmProjects/PythonProject/test_results_improved.txt"
TRAIN_PATH = "/home/he/PycharmProjects/PythonProject/dataset/processed_comcat/train"
VAL_PATH = "/home/he/PycharmProjects/PythonProject/dataset/processed_comcat/val"
TEST_SETS = [
    ("General Test Set", "/home/he/PycharmProjects/PythonProject/dataset/processed_comcat/test"),
]
WAVEFORM_PATH = "/home/he/PycharmProjects/PythonProject/dataset/PNW-ML/comcat_waveforms.hdf5"
SAVE_MODEL_PATH = "/home/he/PycharmProjects/PythonProject/improved_earthquake_classifier.keras"
TRACE_HISTORY_PLOT_PATH = "/home/he/PycharmProjects/PythonProject/trace_training_history.png"
EVENT_HISTORY_PLOT_PATH = "/home/he/PycharmProjects/PythonProject/event_training_history.png"
TRACE_ATTENTION_HEATMAP_PATH = "/home/he/PycharmProjects/PythonProject/trace_weight_.png"
TRACE_PERFORMANCE_PATH = "/home/he/PycharmProjects/PythonProject/trace_performance.png"
CONFUSION_MATRIX_PATH = "/home/he/PycharmProjects/PythonProject/confusion_matrix.png"
ERROR_LOG_PATH = "data_processing_errors.log"
CHECKPOINT_DIR = "/home/he/PycharmProjects/PythonProject/checkpoints"

# ä¿¡å·å¤„ç†å‚æ•°
SAMPLE_RATE = 75
WAVEFORM_DURATION = 60
WAVEFORM_LENGTH = int(SAMPLE_RATE * WAVEFORM_DURATION)
HIGHPASS_FREQ = 2
VALID_COMPONENTS = ["Z"]

# äº‹ä»¶å¤„ç†å‚æ•°
MIN_TRACES_PER_EVENT = 1
MAX_TRACES_PER_EVENT = None

# é¢‘è°±å›¾å‚æ•°
SPECTROGRAM_NPERS = int(2 * SAMPLE_RATE)
SPECTROGRAM_NOVER = int(SPECTROGRAM_NPERS * 0.75)
SPECTROGRAM_FREQ_MIN = 0.5
SPECTROGRAM_FREQ_MAX = 50

# é¢„è®¡ç®—é¢‘è°±å›¾ç»´åº¦
_f, _t, _ = spectrogram(
    x=np.zeros(WAVEFORM_LENGTH),
    fs=SAMPLE_RATE,
    nperseg=SPECTROGRAM_NPERS,
    noverlap=SPECTROGRAM_NOVER,
    nfft=SPECTROGRAM_NPERS
)
_spec_freq_mask = (_f >= SPECTROGRAM_FREQ_MIN) & (_f <= SPECTROGRAM_FREQ_MAX)
SPEC_HEIGHT = int(np.sum(_spec_freq_mask))
SPEC_WIDTH = int(len(_t))
print(f"é¢„è®¡ç®—é¢‘è°±å›¾ç»´åº¦: (é«˜åº¦={SPEC_HEIGHT}, å®½åº¦={SPEC_WIDTH})")
del _f, _t, _spec_freq_mask

# è®­ç»ƒå‚æ•°

BATCH_SIZE = 32
TRACE_BATCH_SIZE = 32
EPOCHS = 400
TRACE_PRETRAIN_EPOCHS = 250
EXPLOSION_WEIGHT_SCALE = 1
LEARNING_RATE = 1e-5
TRACE_LEARNING_RATE = 1e-5
MAX_ERRORS = 1000
ERROR_LOG_INTERVAL = 100
INTERMEDIATE_LOSS_WEIGHT = 0.05
NUM_ATTENTION_HEADS = 4

# åˆ—åæ˜ å°„
COLUMN_MAPPING = {
    "event_id": "event_id",
    "event_type": "event_type",
    "trace_name": "trace_name",
    "trace_P_arrival_sample": "trace_P_arrival_sample",
    "trace_S_arrival_sample": "trace_S_arrival_sample",
    "trace_start_time": "trace_start_time",
    "trace_sampling_rate_hz": "trace_sampling_rate_hz",
    "mag": "mag",
    "source_depth_km": "source_depth_km",
    "origin_time": "origin_time",
    "source_latitude_deg": "source_latitude_deg",
    "source_longitude_deg": "source_longitude_deg",
    "station_latitude_deg": "station_latitude_deg",
    "station_longitude_deg": "station_longitude_deg"
}



# åˆå§‹åŒ–é”™è¯¯æ—¥å¿—
logging.basicConfig(
    filename=ERROR_LOG_PATH,
    level=logging.ERROR,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
error_logger = logging.getLogger("data_processor")

# åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
os.makedirs(CHECKPOINT_DIR, exist_ok=True)


# -------------------------- è‡ªå®šä¹‰å¯åºåˆ—åŒ–Keraså±‚ --------------------------
@tf.keras.utils.register_keras_serializable()
class TraceAttentionLayer(layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.dense1 = layers.Dense(32, activation='relu')
        self.dense2 = layers.Dense(1)

    def call(self, inputs):
        x = self.dense1(inputs)                       # (B, T, 32)
        x = self.dense2(x)                            # (B, T, 1)
        # æ˜¾å¼æŒ‡å®š softmax è½´ï¼Œå¹¶è¿”å›ä¸è¾“å…¥ç›¸åŒçš„ shape
        return tf.nn.softmax(x, axis=1)

    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
class MultiHeadTraceAttention(layers.Layer):
    def __init__(self, num_heads=NUM_ATTENTION_HEADS, **kwargs):
        super().__init__(**kwargs)
        self.num_heads = num_heads
        # æ¯ä¸ªå¤´ç‹¬ç«‹æ‰“åˆ†
        self.attention_heads = [layers.Dense(1, activation='sigmoid') for _ in range(num_heads)]
        self.fuse = layers.Dense(1)  # æŠŠ num_heads ä¸ªåˆ†æ•°èä¸º 1 ä¸ª

    def call(self, inputs, training=None):
        # inputs: (B, T, D)
        head_scores = [head(inputs) for head in self.attention_heads]  # åˆ—è¡¨ï¼Œæ¯ä¸ª (B, T, 1)
        stacked = tf.concat(head_scores, axis=-1)                      # (B, T, num_heads)
        raw_score = self.fuse(stacked)                                # (B, T, 1)
        raw_score = tf.squeeze(raw_score, axis=-1)                    # (B, T)

        # â”€â”€ L1 å½’ä¸€åŒ–ï¼šä¿æŒâ€œåˆ†æ•°è¶Šé«˜è¶Šå¥½â€ä¸”å’Œä¸º 1 â”€â”€
        weight = raw_score / (tf.reduce_sum(raw_score, axis=1, keepdims=True) + 1e-8)
        return tf.expand_dims(weight, -1)  # (B, T, 1)  ä¸æ—§æ¥å£ä¸€è‡´

    def get_config(self):
        return dict(list(super().get_config().items()) + [('num_heads', self.num_heads)])

@tf.keras.utils.register_keras_serializable()
class ExpandWeightsLayer(layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs):
        weights, features = inputs
        # ç”¨ Reshape æ˜¾å¼åŠ è½´ï¼Œé¿å… Lambda æ—  output_shape
        expanded_weights = tf.reshape(weights, tf.concat([tf.shape(weights), [1]], 0))
        return expanded_weights * tf.ones_like(features)

    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
class ExpandLastDim(layers.Layer):
    """æ˜¾å¼æ›¿ä»£ Lambda(lambda x: tf.expand_dims(x, axis=-1))"""

    def call(self, inputs):
        return tf.expand_dims(inputs, axis=-1)

    def compute_output_shape(self, input_shape):
        return input_shape + (1,)

    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
class FusedVectorLayer(layers.Layer):
    """è‡ªå®šä¹‰å±‚ï¼šå°†åŠ æƒåçš„ç‰¹å¾æŒ‰traceç»´åº¦æ±‚å’Œ"""

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs):
        fused = tf.reduce_sum(inputs, axis=1)  # (batch_size, features)
        return fused

    def get_config(self):
        config = super().get_config()
        return config


CUSTOM_OBJECTS = {
    'TraceAttentionLayer': TraceAttentionLayer,
    'MultiHeadTraceAttention': MultiHeadTraceAttention,
    'ExpandWeightsLayer': ExpandWeightsLayer,
    'FusedVectorLayer': FusedVectorLayer,
    'ExpandLastDim': ExpandLastDim,
    'QualityAwareAttentionLayer': QualityAwareAttentionLayer,  # ç¡®ä¿åŒ…å«
    'event_loss': bulletproof_event_loss,
    'trace_loss': bulletproof_trace_loss,
    'weighted_binary_crossentropy': stable_weighted_binary_crossentropy,
    'quality_function': quality_function,
}
# -------------------------- H5FileManager ç±» --------------------------
class H5FileManager:
    def __init__(self, file_path):
        self.file_path = file_path
        self._file = None
        self._is_open = False

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"HDF5æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        if not os.path.isfile(file_path):
            raise IsADirectoryError(f"{file_path} æ˜¯ç›®å½•ï¼Œä¸æ˜¯æ–‡ä»¶")
        if os.path.getsize(file_path) == 0:
            raise ValueError(f"HDF5æ–‡ä»¶ä¸ºç©º: {file_path}")

    def open(self):
        if self._is_open:
            return self._file

        self.close()
        try:
            self._file = h5py.File(self.file_path, "r", swmr=True)
            self._is_open = True
            if len(list(self._file.keys())) == 0:
                raise RuntimeError(f"HDF5æ–‡ä»¶æŸåæˆ–ä¸ºç©º: {self.file_path}")
            return self._file
        except Exception as e:
            raise RuntimeError(f"æ‰“å¼€HDF5æ–‡ä»¶å¤±è´¥: {str(e)}") from e

    def close(self):
        if self._is_open and self._file is not None:
            try:
                self._file.close()
            except Exception as e:
                error_logger.error(f"HDF5å…³é—­é”™è¯¯: {str(e)}")
            finally:
                self._file = None
                self._is_open = False

    def __enter__(self):
        return self.open()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def __del__(self):
        self.close()


# -------------------------- å·¥å…·å‡½æ•° --------------------------
def split_feat_vector(feat_vector):
    """
    æŠŠ 8 ç»´ç‰¹å¾å‘é‡æ‹†æˆä¸‰éƒ¨åˆ†ï¼š
      norm_part : è¦å½’ä¸€åŒ–çš„ 6 ç»´  [frac, mag, log(P/S), hour_sin, hour_cos, weekday]
      dist      : éœ‡ä¸­è·ï¼ˆkmï¼‰â€”â€” ä¸å†å½’ä¸€åŒ–
      depth     : éœ‡æºæ·±åº¦ï¼ˆkmï¼‰â€”â€” ä¸å†å½’ä¸€åŒ–
    """
    norm_part = np.array([feat_vector[0],   # åˆ†å½¢ç»´
                          feat_vector[1],   # éœ‡çº§
                          feat_vector[3],   # log(P/S)
                          feat_vector[5],   # hour_sin
                          feat_vector[6],   # hour_cos
                          feat_vector[7]], dtype=np.float32)  # weekday
    dist  = feat_vector[4]   # éœ‡ä¸­è·
    depth = feat_vector[2]   # éœ‡æºæ·±åº¦
    return norm_part, dist, depth


def merge_feat_vector(norm_part, dist, depth):
    """
    æŠŠ 6 ç»´å½’ä¸€åŒ–éƒ¨åˆ† + åŸå§‹éœ‡ä¸­è· + åŸå§‹æ·±åº¦ é‡æ–°æ‹¼å› 8 ç»´
    é¡ºåºä¸ split_feat_vector ä¸¥æ ¼å¯¹åº”ï¼š
      [frac, mag, depth, log(P/S), dist, hour_sin, hour_cos, weekday]
    """
    return np.array([norm_part[0],  # 0  fractal
                     norm_part[1],  # 1  mag
                     depth,         # 2  depthï¼ˆkmï¼‰â€”â€” æœªå½’ä¸€åŒ–
                     norm_part[2],  # 3  log(P/S)
                     dist,          # 4  distanceï¼ˆkmï¼‰â€”â€” æœªå½’ä¸€åŒ–
                     norm_part[3],  # 5  hour_sin
                     norm_part[4],  # 6  hour_cos
                     norm_part[5]], # 7  weekday
                    dtype=np.float32)

def print_memory_usage(prefix=""):
    try:
        process = psutil.Process(os.getpid())
        mem_used = process.memory_info().rss / (1024 ** 3)
        mem_percent = process.memory_percent()
        print(f"{prefix}å†…å­˜ä½¿ç”¨: {mem_used:.2f} GB ({mem_percent:.1f}%)")
    except Exception:
        pass


@contextmanager
def timing_context(description):
    start_time = time.time()
    try:
        yield
    finally:
        end_time = time.time()
        print(f"{description} è€—æ—¶: {end_time - start_time:.2f} ç§’")


def parse_trace_name(trace_name_str):
    if "$" not in trace_name_str:
        raise ValueError(f"æ— æ•ˆçš„trace_nameæ ¼å¼: {trace_name_str} (ç¼ºå°‘$åˆ†éš”ç¬¦)")
    bucket_part = trace_name_str.split("$")[0]
    event_idx_part = trace_name_str.split("$")[1].split(",")[0]
    try:
        event_idx = int(event_idx_part.strip())
    except ValueError:
        raise ValueError(f"trace_nameä¸­çš„ç´¢å¼•æ— æ•ˆ: {trace_name_str} (ç´¢å¼•éƒ¨åˆ†: {event_idx_part})")
    return f"data/{bucket_part}", event_idx


def get_component_order(h5_file):
    comp_key = "data_format/component_order"
    if comp_key not in h5_file:
        raise KeyError(f"HDF5ç¼ºå°‘å¿…è¦çš„åˆ†é‡é”®: {comp_key} (æ–‡ä»¶ä¸­æ‰€æœ‰é”®: {list(h5_file.keys())})")
    comp_str = h5_file[comp_key][()].decode("utf-8").strip().upper()
    raw_comp = comp_str.split(",") if "," in comp_str else [c for c in comp_str]
    comp_map = {"E": "X", "N": "Y", "Z": "Z"}
    try:
        comp_order = [comp_map[c.strip()] for c in raw_comp]
    except KeyError as e:
        raise ValueError(f"ä¸æ”¯æŒçš„åˆ†é‡: {e} (HDF5ä¸­å¯ç”¨åˆ†é‡: {raw_comp})")
    if VALID_COMPONENTS[0] not in comp_order:
        raise ValueError(
            f"HDF5ä¸­æœªæ‰¾åˆ°ç›®æ ‡åˆ†é‡ {VALID_COMPONENTS[0]} (å¯ç”¨åˆ†é‡: {comp_order})")
    return comp_order


def calculate_arrival_time(start_dt, arrival_sample, sampling_rate):
    if arrival_sample < 0:
        raise ValueError(f"åˆ°è¾¾æ ·æœ¬æ•°ä¸ºè´Ÿ: {arrival_sample}")
    if sampling_rate <= 0:
        raise ValueError(f"æ— æ•ˆçš„é‡‡æ ·ç‡: {sampling_rate} (å¿…é¡»ä¸ºæ­£æ•°)")
    arrival_offset = arrival_sample / sampling_rate
    return start_dt + timedelta(seconds=arrival_offset)


def haversine_distance(lat1, lon1, lat2, lon2):
    """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„åœ°çƒè¡¨é¢è·ç¦»ï¼ˆå…¬é‡Œï¼‰"""
    R = 6371.0  # åœ°çƒåŠå¾„ï¼ˆå…¬é‡Œï¼‰
    lat1_rad = np.radians(lat1)
    lon1_rad = np.radians(lon1)
    lat2_rad = np.radians(lat2)
    lon2_rad = np.radians(lon2)

    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad

    a = np.sin(dlat / 2) ** 2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2) ** 2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

    return R * c


def preprocess_waveform(raw_wave, raw_sr, p_arrival, s_arrival, start_dt, is_training=False):
    if len(raw_wave) == 0:
        raise ValueError("åŸå§‹æ³¢å½¢æ•°æ®ä¸ºç©º")
    if np.all(raw_wave == 0):
        raise ValueError("åŸå§‹æ³¢å½¢å…¨ä¸ºé›¶ (æ— æœ‰æ•ˆä¿¡å·)")
    trace = Trace(
        data=raw_wave.copy(),
        header={"sampling_rate": raw_sr, "starttime": obspy.UTCDateTime(start_dt)}
    )
    trace.detrend("demean").detrend("linear")
    trace.taper(max_percentage=0.05, type="hann")
    nyquist_freq = raw_sr / 2.0
    actual_highpass = min(HIGHPASS_FREQ, nyquist_freq - 0.1)
    if actual_highpass > 0:
        trace.filter('highpass', freq=actual_highpass, corners=4, zerophase=True)
    if trace.stats.sampling_rate != SAMPLE_RATE:
        trace.resample(SAMPLE_RATE, no_filter=True)
    max_amp = np.max(np.abs(trace.data))
    if max_amp > 0:
        trace.data /= max_amp
    else:
        trace.data = np.zeros_like(trace.data)
    p_time_rel = (p_arrival - start_dt).total_seconds()
    start_idx = max(0, int((p_time_rel - 15) * SAMPLE_RATE))
    end_idx = start_idx + WAVEFORM_LENGTH
    if end_idx > len(trace.data):
        end_idx = len(trace.data)
        start_idx = max(0, end_idx - WAVEFORM_LENGTH)
    processed_wave = trace.data[start_idx:end_idx]
    if len(processed_wave) < WAVEFORM_LENGTH:
        pad_length = WAVEFORM_LENGTH - len(processed_wave)
        processed_wave = np.pad(processed_wave, (0, pad_length), mode="constant")

    # ğŸ› ï¸ ä¿®å¤ï¼šåªåœ¨è®­ç»ƒæ—¶æ·»åŠ éšæœºå™ªå£°
    if is_training:
        noise = np.random.normal(0, 0.01, size=processed_wave.shape)
        processed_wave = processed_wave + noise

    sp_delay = (s_arrival - p_arrival).total_seconds()
    if sp_delay < 1.0:
        pg_window = 0.3
        sg_window = 0.6
    elif 1.0 <= sp_delay < 5.0:
        pg_window = 0.4
        sg_window = 0.8
    else:
        pg_window = 0.5
        sg_window = min(1.0, sp_delay * 0.5)
    pg_start_rel = p_time_rel - pg_window
    pg_end_rel = p_time_rel + pg_window
    pg_start_idx = max(0, int(pg_start_rel * SAMPLE_RATE))
    pg_end_idx = min(len(trace.data), int(pg_end_rel * SAMPLE_RATE))
    pg_amp = np.max(np.abs(trace.data[pg_start_idx:pg_end_idx])) if (pg_end_idx > pg_start_idx) else 1e-6
    pg_amp = max(pg_amp, 1e-6)
    sg_start_rel = (s_arrival - start_dt).total_seconds() - sg_window
    sg_end_rel = (s_arrival - start_dt).total_seconds() + sg_window
    sg_start_idx = max(0, int(sg_start_rel * SAMPLE_RATE))
    sg_end_idx = min(len(trace.data), int(sg_end_rel * SAMPLE_RATE))
    sg_amp = np.max(np.abs(trace.data[sg_start_idx:sg_end_idx])) if (sg_end_idx > sg_start_idx) else 1e-6
    sg_amp = max(sg_amp, 1e-6)
    pg_sg_ratio = pg_amp / sg_amp
    pg_sg_ratio = np.clip(pg_sg_ratio, np.exp(-5), np.exp(5))
    log_pg_sg = np.log(pg_sg_ratio)
    if np.isnan(log_pg_sg):
        log_pg_sg = 0.0
    return processed_wave, log_pg_sg


def calculate_spectrogram(waveform_data):
    if len(waveform_data) != WAVEFORM_LENGTH:
        raise RuntimeError(f"æ— æ•ˆçš„æ³¢å½¢é•¿åº¦: é¢„æœŸ {WAVEFORM_LENGTH}, å®é™… {len(waveform_data)}")
    f, t, Sxx = spectrogram(
        x=waveform_data,
        fs=SAMPLE_RATE,
        nperseg=SPECTROGRAM_NPERS,
        noverlap=SPECTROGRAM_NOVER,
        nfft=SPECTROGRAM_NPERS
    )
    freq_mask = (f >= SPECTROGRAM_FREQ_MIN) & (f <= SPECTROGRAM_FREQ_MAX)
    Sxx_filtered = Sxx[freq_mask, :]
    if Sxx_filtered.shape != (SPEC_HEIGHT, SPEC_WIDTH):
        raise RuntimeError(
            f"æ— æ•ˆçš„é¢‘è°±å›¾ç»´åº¦! é¢„æœŸ ({SPEC_HEIGHT},{SPEC_WIDTH}), å®é™… {Sxx_filtered.shape} "
        )
    Sxx_max = np.max(Sxx_filtered)
    if Sxx_max > 0:
        Sxx_filtered /= Sxx_max
    return Sxx_filtered.astype(np.float32)


def calculate_fractal_dimension(waveform):
    n = len(waveform)
    if n < 100:
        return 1.0

    scales = np.logspace(1, np.log10(n // 4), 10, dtype=int)
    scales = scales[scales > 0]
    if len(scales) < 2:
        return 1.0

    Ns = []
    for scale in scales:
        reshaped = waveform[:n // scale * scale].reshape(-1, scale)
        ranges = np.ptp(reshaped, axis=1)
        Ns.append(np.sum(ranges) / scale)

    log_scales = np.log(1.0 / scales)
    log_Ns = np.log(np.maximum(Ns, 1e-6))
    valid_mask = np.isfinite(log_scales) & np.isfinite(log_Ns)
    if np.sum(valid_mask) < 2:
        return 1.0

    coeffs = np.polyfit(log_scales[valid_mask], log_Ns[valid_mask], 1)
    # âœ… é˜²æ­¢ NaN
    if not np.isfinite(coeffs[0]):
        return 1.0
    return np.clip(coeffs[0], 1.0, 2.0)


def parse_time_str(time_str):
    try:
        if "+00:00" in time_str:
            time_str = time_str.split("+00:00")[0].strip()
            return datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S.%f" if "." in time_str else "%Y-%m-%d %H:%M:%S")
        elif "T" in time_str:
            time_str = time_str.replace("Z", "").strip()
            return datetime.strptime(time_str, "%Y-%m-%dT%H:%M:%S.%f" if "." in time_str else "%Y-%m-%dT%H:%M:%S")
        else:
            return datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S.%f" if "." in time_str else "%Y-%m-%d %H:%M:%S")
    except ValueError as e:
        raise ValueError(f"æ—¶é—´è§£æå¤±è´¥: {time_str} (é”™è¯¯: {e})")


def extract_time_features(event_time_str):
    dt = parse_time_str(event_time_str)
    return [
        np.sin(2 * np.pi * dt.hour / 24),
        np.cos(2 * np.pi * dt.hour / 24),
        1 if dt.weekday() < 5 else 0
    ]


def dynamic_padding(traces_list, max_traces):
    """
    0 å‘é‡å¡«å…… + è¿”å› mask
    è¿”å›: (padded_traces, mask)  mask: 1=çœŸå® 0=å¡«å……
    """
    if isinstance(traces_list, np.ndarray):
        traces_list = traces_list.tolist()

    current_count = len(traces_list)
    if current_count >= max_traces:
        return traces_list[:max_traces], np.ones(max_traces, dtype=np.float32)

    if current_count == 0:
        raise ValueError("dynamic_padding è¾“å…¥ä¸ºç©º")

    padded = traces_list.copy()
    mask   = [1.0] * current_count

    # âœ… ç”¨ 0 å‘é‡å¡«å……ï¼Œé¿å…åˆ†å¸ƒæ¼‚ç§»
    zero_sample = np.zeros_like(traces_list[0])
    for _ in range(max_traces - current_count):
        padded.append(zero_sample.copy())
        mask.append(0.0)

    return padded, np.array(mask, dtype=np.float32)

def validate_h5_paths(h5_manager, metadata):
    with h5_manager as h5_file:
        missing_paths = []
        for _, row in metadata.iterrows():
            trace_name = row["trace_name"]
            hdf5_path, _ = parse_trace_name(trace_name)
            if hdf5_path not in h5_file:
                missing_paths.append(hdf5_path)
        if missing_paths:
            unique_missing = set(missing_paths)
            print(f"è­¦å‘Š: å‘ç° {len(unique_missing)} ä¸ªä¸å­˜åœ¨çš„HDF5è·¯å¾„:")
            for path in unique_missing:
                print(f"  - {path}")
        return len(missing_paths) == 0


def check_h5_integrity(file_path):
    try:
        with h5py.File(file_path, 'r') as f:
            buckets = [f"data/bucket{i}" for i in range(1, 11)]
            for bucket in buckets:
                if bucket not in f:
                    print(f"ç¼ºå°‘bucket: {bucket}")
                else:
                    print(f"æ‰¾åˆ°bucket: {bucket}, å½¢çŠ¶: {f[bucket].shape}")
            return True
    except Exception as e:
        print(f"HDF5å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return False


# -------------------------- æ•°æ®åŠ è½½å‡½æ•° --------------------------
def load_metadata_from_split(path):
    metadata_filename = "metadata.csv"
    metadata_path = os.path.join(path, metadata_filename)
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {metadata_path}")

    required_columns = list(COLUMN_MAPPING.values())
    if "event_id" not in required_columns:
        required_columns.append("event_id")

    metadata = pd.read_csv(metadata_path, usecols=required_columns)

    missing_cols = [col for col in required_columns if col not in metadata.columns]
    if missing_cols:
        raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")

    valid_event_types = ["earthquake", "explosion"]
    valid_mask = (
            metadata["event_type"].isin(valid_event_types) &
            metadata["trace_P_arrival_sample"].notna() &
            metadata["trace_S_arrival_sample"].notna() &
            metadata["trace_start_time"].notna() &
            metadata["trace_sampling_rate_hz"].notna() &
            metadata["mag"].notna() &
            metadata["source_depth_km"].notna() &
            metadata["origin_time"].notna() &
            metadata["source_latitude_deg"].notna() &
            metadata["source_longitude_deg"].notna() &
            metadata["station_latitude_deg"].notna() &
            metadata["station_longitude_deg"].notna()
    )
    metadata = metadata[valid_mask].copy()
    if len(metadata) == 0:
        raise ValueError(f"{path} ä¸­æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")

    event_dist = metadata["event_type"].value_counts().to_dict()
    print(
        f"å·²åŠ è½½ {path}: å…± {len(metadata)} ä¸ªæœ‰æ•ˆæ ·æœ¬ (åˆ†å¸ƒ: åœ°éœ‡={event_dist.get('earthquake', 0)}, çˆ†ç‚¸={event_dist.get('explosion', 0)})")

    reverse_mapping = {v: k for k, v in COLUMN_MAPPING.items()}
    metadata_renamed = metadata.rename(columns=reverse_mapping)

    if "event_id" not in metadata_renamed.columns and "event_id" in metadata.columns:
        metadata_renamed["event_id"] = metadata["event_id"]

    return metadata_renamed


def group_metadata_by_event(metadata):
    """
    æŒ‰äº‹ä»¶IDåˆ†ç»„å…ƒæ•°æ® - ä¿ç•™å•traceäº‹ä»¶
    """
    global MAX_TRACES_PER_EVENT

    event_id_groups = metadata.groupby("event_id").groups
    all_events = []

    # è¯¦ç»†ç»Ÿè®¡
    total_events = len(event_id_groups)
    events_discarded = 0
    trace_count_distribution = {}

    print(f"\n=== äº‹ä»¶åˆ†ç»„è¯¦ç»†ç»Ÿè®¡ (ä¿ç•™å•traceäº‹ä»¶) ===")
    print(f"åŸå§‹äº‹ä»¶æ€»æ•°: {total_events}")

    for event_id, trace_indices in event_id_groups.items():
        traces = metadata.loc[trace_indices]
        event_type = traces["event_type"].iloc[0]
        trace_count = len(traces)

        # è®°å½•traceæ•°é‡åˆ†å¸ƒ
        if trace_count not in trace_count_distribution:
            trace_count_distribution[trace_count] = 0
        trace_count_distribution[trace_count] += 1

        # å…³é”®ä¿®æ”¹ï¼šä¿ç•™æ‰€æœ‰äº‹ä»¶ï¼ŒåŒ…æ‹¬å•traceäº‹ä»¶
        all_events.append((event_type, traces))

    # åŠ¨æ€è®¾ç½® MAX_TRACES_PER_EVENTï¼ˆåŸºäºæ‰€æœ‰äº‹ä»¶ï¼‰
    if all_events:
        trace_counts = [len(traces) for _, traces in all_events]
        proposed_max = int(np.percentile(trace_counts, 98))
        # ç¡®ä¿è‡³å°‘èƒ½å¤„ç†å•traceäº‹ä»¶
        MAX_TRACES_PER_EVENT = max(1, proposed_max)
    else:
        MAX_TRACES_PER_EVENT = 1  # æœ€å°å€¼ä¸º1

    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print(f"ä¿ç•™çš„äº‹ä»¶æ€»æ•°: {len(all_events)}")
    print(f"è¢«ä¸¢å¼ƒçš„äº‹ä»¶æ•°: {events_discarded}")
    print(f"åˆ©ç”¨ç‡: {len(all_events) / total_events * 100:.1f}%")
    print(f"åŠ¨æ€è®¾ç½® MAX_TRACES_PER_EVENT = {MAX_TRACES_PER_EVENT}")

    # æ‰“å°traceæ•°é‡åˆ†å¸ƒ
    print(f"\nTraceæ•°é‡åˆ†å¸ƒ:")
    for count in sorted(trace_count_distribution.keys()):
        events_with_count = trace_count_distribution[count]
        percentage = events_with_count / total_events * 100
        print(f"  {count} traces: {events_with_count} äº‹ä»¶ ({percentage:.1f}%) [å…¨éƒ¨ä¿ç•™]")

    # äº‹ä»¶ç±»å‹åˆ†å¸ƒ
    event_types = [event_type for event_type, _ in all_events]
    unique_types, counts = np.unique(event_types, return_counts=True)
    type_dist = dict(zip(unique_types, counts))
    print(f"\næœ€ç»ˆäº‹ä»¶ç±»å‹åˆ†å¸ƒ: {type_dist}")

    np.random.shuffle(all_events)
    return all_events

def is_valid_trace(waveform):
    """
    æ£€æŸ¥æ³¢å½¢æ•°æ®æ˜¯å¦æœ‰æ•ˆ
    """
    if len(waveform) == 0:
        return False
    if np.all(waveform == 0):
        return False
    if np.std(waveform) < 1e-8:  # å‡ ä¹æ’å®šçš„ä¿¡å·
        return False
    if np.any(np.isnan(waveform)) or np.any(np.isinf(waveform)):
        return False
    return True


def trace_generator(metadata, h5_manager, scaler=None, shuffle=False, is_training=False):
    """ç”Ÿæˆå•ä¸ª trace çš„æ ·æœ¬ - ä¿®å¤ç‰¹å¾ç»´åº¦é—®é¢˜"""
    if shuffle:
        metadata = metadata.sample(frac=1, random_state=42).reset_index(drop=True)

    total_samples = len(metadata)
    error_count = 0
    success_count = 0
    empty_trace_count = 0

    with h5_manager as h5_file:
        comp_order_cache = None
        for idx, (_, row) in enumerate(metadata.iterrows()):
            if idx % 1000 == 0:
                print(
                    f"å¤„ç†è¿›åº¦: {idx}/{total_samples} (æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}, ç©ºtrace: {empty_trace_count})")

            trace_name = row["trace_name"]
            try:
                if comp_order_cache is None:
                    comp_order_cache = get_component_order(h5_file)
                target_comp_idx = comp_order_cache.index(VALID_COMPONENTS[0])
                hdf5_path, event_idx = parse_trace_name(trace_name)

                if hdf5_path not in h5_file:
                    error_count += 1
                    continue

                wave_group = h5_file[hdf5_path]
                if event_idx < 0 or event_idx >= wave_group.shape[0]:
                    error_count += 1
                    continue

                raw_wave = wave_group[event_idx, target_comp_idx, :].copy()
                if not is_valid_trace(raw_wave):
                    empty_trace_count += 1
                    continue

                start_dt = parse_time_str(row["trace_start_time"])
                raw_sr = float(row["trace_sampling_rate_hz"])
                p_arrival = calculate_arrival_time(start_dt, int(row["trace_P_arrival_sample"]), raw_sr)
                s_arrival = calculate_arrival_time(start_dt, int(row["trace_S_arrival_sample"]), raw_sr)
                #ä¿®å¤ï¼šä¼ å…¥ is_training å‚æ•°
                processed_wave, log_pg_sg = preprocess_waveform(raw_wave, raw_sr, p_arrival, s_arrival, start_dt, is_training)
                del raw_wave

                if len(processed_wave) != WAVEFORM_LENGTH:
                    error_count += 1
                    continue

                spec_data = calculate_spectrogram(processed_wave)
                processed_wave = np.expand_dims(processed_wave, axis=-1)
                spec_data = np.expand_dims(spec_data, axis=-1)

                fractal_dim = calculate_fractal_dimension(processed_wave[:, 0])
                epicentral_distance = haversine_distance(
                    row["source_latitude_deg"], row["source_longitude_deg"],
                    row["station_latitude_deg"], row["station_longitude_deg"]
                )
                time_features = extract_time_features(row["origin_time"])

                #  ä¿®å¤ï¼šæ„å»º8ç»´ç‰¹å¾å‘é‡ï¼ˆå•Traceè®­ç»ƒé˜¶æ®µï¼‰
                feat_vector = np.array([
                    fractal_dim,
                    float(row["mag"]),
                    float(row["source_depth_km"]),
                    log_pg_sg,
                    epicentral_distance,
                    time_features[0],
                    time_features[1],
                    time_features[2]
                ], dtype=np.float32)

                # å½’ä¸€åŒ–å¤„ç†
                if scaler is not None:
                    norm_part, dist, depth = split_feat_vector(feat_vector)
                    norm_part = scaler.transform(norm_part.reshape(1, -1)).flatten()
                    feat_vector = merge_feat_vector(norm_part, dist, depth)

                label = 1 if row["event_type"] == "earthquake" else 0

                success_count += 1
                #  ä¿®å¤ï¼šè¾“å‡º8ç»´ç‰¹å¾å‘é‡
                yield (processed_wave, spec_data, feat_vector), np.array(label, dtype=np.int8)

            except Exception as e:
                error_count += 1
                continue

    print(f"å¤„ç†å®Œæˆ: å…± {success_count} ä¸ªæˆåŠŸ, {error_count} ä¸ªå¤±è´¥, {empty_trace_count} ä¸ªç©ºtrace")


def event_generator(event_groups, h5_path, scaler=None, shuffle=False, trace_prob_cache=None, is_training=False):
    """äº‹ä»¶ç”Ÿæˆå™¨ï¼šä¿®å¤ç‰¹å¾ç»´åº¦é—®é¢˜"""
    if trace_prob_cache is None:
        raise RuntimeError("å¿…é¡»ä¼ å…¥ trace_prob_cacheï¼")
    if shuffle:
        event_groups = event_groups.copy()
        np.random.shuffle(event_groups)

    with h5py.File(h5_path, 'r', swmr=True) as h5_file:
        comp_order_cache = None
        for event_type, traces in event_groups:
            event_waves, event_specs, event_feats = [], [], []
            valid_traces_count = 0

            for _, row in traces.iterrows():
                try:
                    trace_name = row["trace_name"]
                    if comp_order_cache is None:
                        comp_order_cache = get_component_order(h5_file)
                    target_comp_idx = comp_order_cache.index(VALID_COMPONENTS[0])
                    hdf5_path, event_idx = parse_trace_name(trace_name)

                    if hdf5_path not in h5_file:
                        continue

                    wave_group = h5_file[hdf5_path]

                    if event_idx < 0 or event_idx >= wave_group.shape[0]:
                        continue

                    raw_wave = wave_group[event_idx, target_comp_idx, :].copy()

                    if not is_valid_trace(raw_wave):
                        continue

                    # å¤„ç†æ³¢å½¢æ•°æ®
                    start_dt = parse_time_str(row["trace_start_time"])
                    raw_sr = float(row["trace_sampling_rate_hz"])
                    p_arrival = calculate_arrival_time(start_dt, int(row["trace_P_arrival_sample"]), raw_sr)
                    s_arrival = calculate_arrival_time(start_dt, int(row["trace_S_arrival_sample"]), raw_sr)
                    #  ä¿®å¤ï¼šä¼ å…¥ is_training å‚æ•°
                    processed_wave, log_pg_sg = preprocess_waveform(raw_wave, raw_sr, p_arrival, s_arrival, start_dt, is_training)
                    spec_data = calculate_spectrogram(processed_wave)
                    processed_wave = np.expand_dims(processed_wave, axis=-1)
                    spec_data = np.expand_dims(spec_data, axis=-1)

                    fractal_dim = calculate_fractal_dimension(processed_wave[:, 0])
                    epicentral_distance = haversine_distance(
                        row["source_latitude_deg"], row["source_longitude_deg"],
                        row["station_latitude_deg"], row["station_longitude_deg"])
                    time_features = extract_time_features(row["origin_time"])

                    #  ä¿®å¤ï¼šæ„å»º8ç»´ç‰¹å¾å‘é‡
                    feat_vector = np.array([
                        fractal_dim,
                        float(row["mag"]),
                        float(row["source_depth_km"]),
                        log_pg_sg,
                        epicentral_distance,
                        time_features[0],
                        time_features[1],
                        time_features[2]
                    ], dtype=np.float32)

                    # å½’ä¸€åŒ–å¤„ç†
                    if scaler is not None:
                        norm_part, dist, depth = split_feat_vector(feat_vector)
                        norm_part = scaler.transform(norm_part.reshape(1, -1)).flatten()
                        feat_vector = merge_feat_vector(norm_part, dist, depth)

                    # è·å–traceæ¨¡å‹é¢„æµ‹æ¦‚ç‡
                    if trace_name not in trace_prob_cache:
                        continue

                    trace_prob = trace_prob_cache[trace_name]

                    # è®¡ç®—è´¨é‡åˆ†æ•°
                    true_label = 1 if row["event_type"] == "earthquake" else 0
                    quality_score = float((2 * true_label - 1) * (2 * trace_prob - 1))

                    # ğŸ› ï¸ ä¿®å¤ï¼šæ„å»º10ç»´ç‰¹å¾å‘é‡ï¼ˆ8ç»´åŸå§‹ç‰¹å¾ + traceæ¦‚ç‡ + è´¨é‡åˆ†æ•°ï¼‰
                    enhanced_feat_vector = np.append(feat_vector, [trace_prob, quality_score])

                    event_waves.append(processed_wave)
                    event_specs.append(spec_data)
                    event_feats.append(enhanced_feat_vector)
                    valid_traces_count += 1

                    del processed_wave, spec_data, raw_wave

                except Exception as e:
                    continue

            # åªè¦æœ‰æœ‰æ•ˆtraceå°±å¤„ç†
            if valid_traces_count > 0:
                event_waves_np = np.stack(event_waves, axis=0)
                event_specs_np = np.stack(event_specs, axis=0)
                event_feats_np = np.stack(event_feats, axis=0)
                event_label = 1 if event_type == "earthquake" else 0
                trace_labels = np.full((valid_traces_count,), event_label, dtype=np.int8)

                yield (event_waves_np, event_specs_np, event_feats_np), \
                    (np.int8(event_label), trace_labels)


def build_trace_tf_dataset(metadata, h5_manager, scaler=None, shuffle=False, batch_size=TRACE_BATCH_SIZE, is_training=False):
    """æ„å»ºå•traceçš„TFæ•°æ®é›† - ä¿®å¤ç‰¹å¾ç»´åº¦é—®é¢˜"""

    #  ä¿®å¤ï¼šè¾“å‡ºå½¢çŠ¶è°ƒæ•´ï¼šç‰¹å¾ç»´åº¦æ”¹ä¸º8
    output_types = ((tf.float32, tf.float32, tf.float32), tf.int8)
    output_shapes = (
        (
            tf.TensorShape([WAVEFORM_LENGTH, 1]),
            tf.TensorShape([SPEC_HEIGHT, SPEC_WIDTH, 1]),
            tf.TensorShape([8])  # æ”¹ä¸º8ç»´!
        ),
        tf.TensorShape([])
    )

    def generator_factory():
        return trace_generator(metadata, h5_manager, scaler, shuffle, is_training)

    dataset = tf.data.Dataset.from_generator(
        generator_factory,
        output_types=output_types,
        output_shapes=output_shapes
    )
    dataset = dataset.batch(batch_size, drop_remainder=False)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset


def build_event_tf_dataset(event_groups, h5_path, scaler=None, shuffle=False,
                           batch_size=BATCH_SIZE, trace_prob_cache=None, is_training=False):
    """æ„å»ºäº‹ä»¶çº§TFæ•°æ®é›†ï¼šä¿®å¤å¡«å……é€»è¾‘"""

    max_tr = MAX_TRACES_PER_EVENT or 10
    print(f"[DEBUG] æ„å»ºäº‹ä»¶æ•°æ®é›†: {len(event_groups)} ä¸ªäº‹ä»¶, batch_size={batch_size}")

    if trace_prob_cache is None:
        raise RuntimeError("å¿…é¡»ä¼ å…¥ trace_prob_cacheï¼")

    def gen():
        for (waves, specs, feats), (y_event, y_trace) in event_generator(
                event_groups=event_groups,
                h5_path=h5_path,
                scaler=scaler,
                shuffle=shuffle,
                trace_prob_cache=trace_prob_cache,
                is_training=is_training  # ğŸ› ï¸ ä¿®å¤ï¼šä¼ å…¥ is_training å‚æ•°
        ):
            n_real = waves.shape[0]

            # ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰æ•°ç»„éƒ½æœ‰æ­£ç¡®çš„å½¢çŠ¶
            if n_real > max_tr:
                # å¦‚æœtraceæ•°é‡è¶…è¿‡æœ€å¤§å€¼ï¼Œæˆªæ–­
                waves = waves[:max_tr]
                specs = specs[:max_tr]
                feats = feats[:max_tr]
                y_trace = y_trace[:max_tr]
            elif n_real < max_tr:
                # å¦‚æœtraceæ•°é‡ä¸è¶³ï¼Œå¡«å……
                pad = max_tr - n_real

                # è·å–æ­£ç¡®çš„å¡«å……å½¢çŠ¶
                wave_pad_shape = ((0, pad), (0, 0), (0, 0))
                spec_pad_shape = ((0, pad), (0, 0), (0, 0), (0, 0))
                feat_pad_shape = ((0, pad), (0, 0))

                waves = np.pad(waves, wave_pad_shape, 'constant')
                specs = np.pad(specs, spec_pad_shape, 'constant')
                feats = np.pad(feats, feat_pad_shape, 'constant')
                y_trace = np.pad(y_trace, (0, pad), constant_values=-1)

            # ä¿®å¤ï¼šç¡®ä¿è¿”å›æ­£ç¡®çš„æ•°æ®ç±»å‹
            yield (waves.astype(np.float32),
                   specs.astype(np.float32),
                   feats.astype(np.float32)), \
                (np.int8(y_event), y_trace.astype(np.int8))

    # è¾“å‡ºç±»å‹å’Œå½¢çŠ¶
    output_types = (
        (tf.float32, tf.float32, tf.float32),
        (tf.int8, tf.int8)
    )
    output_shapes = (
        (tf.TensorShape([None, WAVEFORM_LENGTH, 1]),
         tf.TensorShape([None, SPEC_HEIGHT, SPEC_WIDTH, 1]),
         tf.TensorShape([None, 10])),
        (tf.TensorShape([]),
         tf.TensorShape([None]))
    )

    dataset = tf.data.Dataset.from_generator(
        gen, output_types=output_types, output_shapes=output_shapes
    )
    dataset = dataset.batch(batch_size, drop_remainder=False)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    # ğŸ› ï¸ ä¿®å¤ï¼šæ·»åŠ repeat()é¿å…æ•°æ®è€—å°½
    dataset = dataset.repeat()

    return dataset

# -------------------------- å­¦ä¹ ç‡è°ƒåº¦å™¨ --------------------------
def _create_trace_encoder(self):
    """å…±äº« Trace ç¼–ç å™¨ï¼šç‰¹å¾åˆ†æ”¯å·²é€‚é… 10 ç»´è¾“å…¥"""
    # æ³¢å½¢åˆ†æ”¯
    wave_in = layers.Input(shape=(WAVEFORM_LENGTH, 1))
    x = layers.Conv1D(32, 5, activation='relu', padding='same')(wave_in)
    x = layers.MaxPooling1D(2)(x)
    x = layers.Conv1D(64, 5, activation='relu', padding='same')(x)
    x = layers.GlobalAveragePooling1D()(x)
    wave_enc = keras.Model(wave_in, x)

    # é¢‘è°±å›¾åˆ†æ”¯
    spec_in = layers.Input(shape=(SPEC_HEIGHT, SPEC_WIDTH, 1))
    y = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(spec_in)
    y = layers.MaxPooling2D((2, 2))(y)
    y = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(y)
    y = layers.GlobalAveragePooling2D()(y)
    spec_enc = keras.Model(spec_in, y)

    # ç‰¹å¾åˆ†æ”¯ï¼ˆ10 ç»´ï¼‰
    feat_in = layers.Input(shape=(10,))
    z = layers.Dense(32, activation='relu')(feat_in)
    z = layers.Dense(32, activation='relu')(z)
    feat_enc = keras.Model(feat_in, z)

    return wave_enc, spec_enc, feat_enc


def create_trace_adaptive_scheduler(initial_lr):
    """ä¸ºå•traceæ¨¡å‹åˆ›å»ºè‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å™¨"""

    def lr_scheduler(epoch):
        if epoch < 15:
            return initial_lr
        elif epoch < 30:
            return initial_lr * 0.5
        else:
            return initial_lr * 0.2

    return lr_scheduler


# -------------------------- å›è°ƒå‡½æ•° --------------------------
class ExplosionRecallLogger(Callback):
    def __init__(self, val_dataset, val_steps, is_trace_model=False):
        super().__init__()
        self.val_dataset = val_dataset
        self.val_steps = val_steps
        self.is_trace_model = is_trace_model

    def on_epoch_end(self, epoch, logs=None):
        y_true = []
        y_pred_prob = []

        if self.is_trace_model:
            model_output = self.model.output
        else:
            model_output = self.model.output[0]  # äº‹ä»¶çº§è¾“å‡ºæ˜¯ç¬¬ä¸€ä¸ª

        try:
            if self.is_trace_model:
                for (x1, x2, x3), y_label in self.val_dataset.take(self.val_steps):
                    pred = self.model.predict([x1, x2, x3], verbose=0)
                    y_true.extend(y_label.numpy().tolist())
                    y_pred_prob.extend(pred.flatten().tolist())
            else:
                # ä¿®å¤ï¼šç°åœ¨åªæœ‰ä¸¤ä¸ªè¾“å‡º
                for (x1, x2, x3), (y_event, y_trace) in self.val_dataset.take(self.val_steps):
                    pred = self.model.predict([x1, x2, x3], verbose=0)
                    pred = pred[0]  # äº‹ä»¶çº§è¾“å‡ºæ˜¯ç¬¬ä¸€ä¸ª
                    y_true.extend(y_event.numpy().tolist())
                    y_pred_prob.extend(pred.flatten().tolist())
        except ValueError as e:
            print(f"æ•°æ®è§£åŒ…é”™è¯¯: {str(e)}")
            return

        if len(y_true) == 0:
            logs['val_explosion_recall'] = 0.0
            return

        y_pred = (np.array(y_pred_prob) > 0.5).astype(int)
        cm = confusion_matrix(y_true, y_pred)

        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            explosion_recall = tn / (tn + fp) if (tn + fp) > 0 else 0.0
            earthquake_recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        else:
            explosion_recall = 0.0
            earthquake_recall = 0.0

        if logs is not None:
            logs['val_explosion_recall'] = explosion_recall
            logs['val_earthquake_recall'] = earthquake_recall
            print(f" - Validation explosion recall: {explosion_recall:.4f}, earthquake recall: {earthquake_recall:.4f}")
# -------------------------- ä¿®å¤è´Ÿ loss çš„å›è°ƒ --------------------------
class FixTotalLossCallback(Callback):
    """
    æŠŠ logs['loss'] ä¿®æ­£ä¸º
        1.0 * event_output_loss + INTERMEDIATE_LOSS_WEIGHT * trace_classifier_loss
    è¦†ç›– TensorFlow æ—¥å¿—é‡Œçš„è´Ÿå€¼ bugã€‚
    """
    def __init__(self):
        super().__init__()

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            return

        ev_loss = logs.get('event_output_loss')
        tr_loss = logs.get('trace_classifier_loss')

        # æµ®ç‚¹å®¹é”™ + å¸¸é‡å­˜åœ¨æ€§æ£€æŸ¥
        if (ev_loss is not None and tr_loss is not None and
            np.isfinite(ev_loss) and np.isfinite(tr_loss)):
            logs['loss'] = ev_loss + INTERMEDIATE_LOSS_WEIGHT * tr_loss
            # å¯é€‰ï¼šæ‰“å°ç¡®è®¤ï¼ˆç¬¬ä¸€æ¬¡ epoch æˆ–æ¯ 10 æ¬¡ï¼‰
            if epoch == 0 or (epoch + 1) % 10 == 0:
                print(f" - FixTotalLossCallback: ä¿®æ­£å total loss = {logs['loss']:.4f}")
class MemoryCleaner(Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()
        print_memory_usage("è®­ç»ƒå ")


class TracePerformanceLogger(Callback):
    """ä¿®å¤çš„Traceæ€§èƒ½è®°å½•å™¨"""

    def __init__(self, val_dataset, val_steps):
        super().__init__()
        self.val_dataset = val_dataset
        self.val_steps = val_steps

    def on_epoch_end(self, epoch, logs=None):
        y_true = []
        y_pred_prob = []

        try:
            for (x1, x2, x3), (y_event, y_trace) in self.val_dataset.take(self.val_steps):
                preds = self.model.predict([x1, x2, x3], verbose=0)

                # ğŸ› ï¸ ä¿®å¤ï¼šæ­£ç¡®è§£åŒ…è¾“å‡º
                if isinstance(preds, list) and len(preds) >= 2:
                    trace_pred = preds[1]  # traceçº§è¾“å‡ºæ˜¯ç¬¬äºŒä¸ª
                else:
                    trace_pred = preds  # å¦‚æœæ˜¯å•ä¸€è¾“å‡º

                batch_size = y_event.shape[0]
                for b in range(batch_size):
                    # è®¡ç®—çœŸå®traceæ•°é‡ï¼ˆæ’é™¤å¡«å……çš„-1ï¼‰
                    real_mask = y_trace[b] != -1
                    n_real = tf.reduce_sum(tf.cast(real_mask, tf.int32)).numpy()

                    if n_real == 0:
                        continue

                    # æå–çœŸå®traceçš„é¢„æµ‹å’Œæ ‡ç­¾
                    trace_pred_real = trace_pred[b, :n_real, 0] if len(trace_pred.shape) == 3 else trace_pred[
                        b, :n_real]
                    y_trace_real = y_trace[b, :n_real]

                    y_true.extend(y_trace_real.numpy().tolist())
                    y_pred_prob.extend(trace_pred_real.flatten().tolist())

            if len(y_true) == 0:
                print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„traceæ ·æœ¬ç”¨äºè®¡ç®—æŒ‡æ ‡")
                logs['val_trace_accuracy'] = 0.0
                logs['val_trace_precision'] = 0.0
                logs['val_trace_recall'] = 0.0
                return

            y_pred = (np.array(y_pred_prob) > 0.5).astype(int)
            accuracy = np.mean(np.array(y_true) == y_pred)

            # è®¡ç®—precisionå’Œrecallï¼ˆåªåœ¨æœ‰æ­£æ ·æœ¬æ—¶ï¼‰
            if np.sum(y_true) > 0 and np.sum(y_pred) > 0:
                cm = confusion_matrix(y_true, y_pred)
                if cm.shape == (2, 2):
                    tn, fp, fn, tp = cm.ravel()
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                else:
                    precision = 0.0
                    recall = 0.0
            else:
                precision = 0.0
                recall = 0.0

            if logs is not None:
                logs['val_trace_accuracy'] = accuracy
                logs['val_trace_precision'] = precision
                logs['val_trace_recall'] = recall
                print(f" - Validation trace accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}")

        except Exception as e:
            print(f"Traceæ€§èƒ½è®°å½•å™¨é”™è¯¯: {e}")
            if logs is not None:
                logs['val_trace_accuracy'] = 0.0
                logs['val_trace_precision'] = 0.0
                logs['val_trace_recall'] = 0.0


# -------------------------- æ¨¡å‹ç±» --------------------------
class EarthquakeClassifier:
    def __init__(self, column_mapping):
        self.model = None
        self.trace_model = None
        self.scaler = StandardScaler()
        self.column_mapping = column_mapping
        self.h5_manager = None
        self.event_class_weights = {0: 1.0, 1: 1.0}
        os.makedirs(os.path.dirname(RESULT_OUTPUT_PATH), exist_ok=True)
        self.trace_prob_cache = None

    def validate_shapes(self, dataset, steps=1):
        """éªŒè¯è¾“å…¥è¾“å‡ºå½¢çŠ¶"""
        print("éªŒè¯æ•°æ®é›†å½¢çŠ¶:")
        try:
            for i, (inputs, outputs) in enumerate(dataset.take(steps)):
                x1, x2, x3 = inputs
                y_event, y_trace = outputs

                print(f"æ‰¹æ¬¡ {i + 1}:")
                print(f"  è¾“å…¥å½¢çŠ¶: wave={x1.shape}, spec={x2.shape}, feat={x3.shape}")
                print(f"  è¾“å‡ºå½¢çŠ¶: event_label={y_event.shape}, trace_label={y_trace.shape}")

                # éªŒè¯æ¨¡å‹è¾“å‡ºå½¢çŠ¶
                if self.model is not None:
                    try:
                        pred_event, pred_trace = self.model.predict([x1, x2, x3], verbose=0)
                        print(f"  æ¨¡å‹è¾“å‡ºå½¢çŠ¶: event_pred={pred_event.shape}, trace_pred={pred_trace.shape}")

                        # éªŒè¯æŸå¤±è®¡ç®—
                        loss = self.model.test_on_batch([x1, x2, x3], [y_event, y_trace])
                        print(f"  æµ‹è¯•æŸå¤±: {loss}")
                    except Exception as e:
                        print(f"  æ¨¡å‹é¢„æµ‹å¤±è´¥: {e}")
                print("-" * 50)
        except Exception as e:
            print(f"å½¢çŠ¶éªŒè¯å¤±è´¥: {e}")

    def build_trace_model(self):
        """é«˜å‡†ç¡®ç‡ç‰ˆTraceæ¨¡å‹ - ä¿®å¤ç‰¹å¾ç»´åº¦é—®é¢˜"""
        waveform_input = layers.Input(shape=(WAVEFORM_LENGTH, 1), name="waveform_input")
        x = layers.Conv1D(16, 5, activation="relu", padding="same")(waveform_input)
        x = layers.MaxPooling1D(2, padding="same")(x)
        x = layers.BatchNormalization()(x)
        x = layers.Conv1D(32, 5, activation="relu", padding="same")(x)
        x = layers.MaxPooling1D(2, padding="same")(x)
        x = layers.BatchNormalization()(x)
        x = layers.Conv1D(64, 5, activation="relu", padding="same")(x)
        x = layers.MaxPooling1D(2, padding="same")(x)
        x = layers.BatchNormalization()(x)
        x = layers.GlobalAveragePooling1D()(x)
        waveform_branch = layers.Dense(32, activation="relu", name="wave_embed")(x)

        spectrogram_input = layers.Input(shape=(SPEC_HEIGHT, SPEC_WIDTH, 1), name="spectrogram_input")
        y = layers.Conv2D(16, (3, 3), activation="relu", padding="same")(spectrogram_input)
        y = layers.MaxPooling2D((2, 2), padding="same")(y)
        y = layers.BatchNormalization()(y)
        y = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(y)
        y = layers.MaxPooling2D((2, 2), padding="same")(y)
        y = layers.BatchNormalization()(y)
        y = layers.Conv2D(48, (3, 3), activation="relu", padding="same")(y)
        y = layers.MaxPooling2D((2, 2), padding="same")(y)
        y = layers.BatchNormalization()(y)
        y = layers.GlobalAveragePooling2D()(y)
        spectrogram_branch = layers.Dense(32, activation="relu", name="spec_embed")(y)

        #  ä¿®å¤ï¼šç‰¹å¾è¾“å…¥æ”¹ä¸º8ç»´ï¼ˆé¢„è®¡ç®—é˜¶æ®µåªæœ‰8ç»´ç‰¹å¾ï¼‰
        features_input = layers.Input(shape=(8,), name="features_input")  # æ”¹ä¸º8ç»´!
        z = layers.Dense(32, activation="relu")(features_input)
        z = layers.BatchNormalization()(z)
        features_branch = layers.Dense(32, activation="relu", name="feat_embed")(z)

        combined = layers.concatenate([waveform_branch, spectrogram_branch, features_branch])
        combined = layers.Dense(64, activation="relu")(combined)
        combined = layers.BatchNormalization()(combined)
        combined = layers.Dropout(0.5)(combined)
        output = layers.Dense(1, activation="sigmoid", name="trace_output")(combined)

        self.trace_model = keras.Model(
            inputs=[waveform_input, spectrogram_input, features_input],
            outputs=output
        )

        # ä½¿ç”¨å®‰å…¨ä¼˜åŒ–å™¨å’Œé˜²å¼¹æŸå¤±å‡½æ•°
        optimizer = build_safe_optimizer(TRACE_LEARNING_RATE)
        self.trace_model.compile(
            optimizer=optimizer,
            loss=bulletproof_event_loss,
            metrics=["accuracy", "precision", "recall"]
        )
        return self.trace_model

    def precompute_trace_probs(self, metadata, h5_manager):
        """é¢„è®¡ç®—traceæ¦‚ç‡ - ä¿®å¤ç‰¹å¾ç»´åº¦é—®é¢˜"""
        print("å¼€å§‹é¢„è®¡ç®— trace æ¦‚ç‡...")
        probs = {}

        #  ä¿®å¤ï¼šé€æ¡å¤„ç†ï¼Œç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
        total_traces = len(metadata)
        success_count = 0
        error_count = 0
        invalid_trace_count = 0

        with h5_manager as h5_file:
            comp_order_cache = None

            for idx, (_, row) in enumerate(metadata.iterrows()):
                if idx % 1000 == 0:
                    print(
                        f"é¢„è®¡ç®—è¿›åº¦: {idx}/{total_traces} (æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}, æ— æ•ˆ: {invalid_trace_count})")

                trace_name = row["trace_name"]

                try:
                    if comp_order_cache is None:
                        comp_order_cache = get_component_order(h5_file)
                    target_comp_idx = comp_order_cache.index(VALID_COMPONENTS[0])
                    hdf5_path, event_idx = parse_trace_name(trace_name)

                    if hdf5_path not in h5_file:
                        error_count += 1
                        if error_count <= 10:
                            print(f"é”™è¯¯: HDF5è·¯å¾„ä¸å­˜åœ¨: {hdf5_path}, trace: {trace_name}")
                        continue

                    wave_group = h5_file[hdf5_path]
                    if event_idx < 0 or event_idx >= wave_group.shape[0]:
                        error_count += 1
                        if error_count <= 10:
                            print(
                                f"é”™è¯¯: äº‹ä»¶ç´¢å¼•è¶…å‡ºèŒƒå›´: {event_idx}, æœ€å¤§ç´¢å¼•: {wave_group.shape[0] - 1}, trace: {trace_name}")
                        continue

                    raw_wave = wave_group[event_idx, target_comp_idx, :].copy()

                    # æ£€æŸ¥æ³¢å½¢æœ‰æ•ˆæ€§
                    if not is_valid_trace(raw_wave):
                        invalid_trace_count += 1
                        continue

                    # é¢„å¤„ç†æ³¢å½¢
                    start_dt = parse_time_str(row["trace_start_time"])
                    raw_sr = float(row["trace_sampling_rate_hz"])
                    p_arrival = calculate_arrival_time(start_dt, int(row["trace_P_arrival_sample"]), raw_sr)
                    s_arrival = calculate_arrival_time(start_dt, int(row["trace_S_arrival_sample"]), raw_sr)
                    # ğŸ› ï¸ ä¿®å¤ï¼šé¢„è®¡ç®—æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
                    processed_wave, log_pg_sg = preprocess_waveform(raw_wave, raw_sr, p_arrival, s_arrival, start_dt,
                                                                    is_training=False)

                    # å†æ¬¡æ£€æŸ¥å¤„ç†åçš„æ³¢å½¢
                    if len(processed_wave) != WAVEFORM_LENGTH:
                        invalid_trace_count += 1
                        continue

                    spec_data = calculate_spectrogram(processed_wave)
                    processed_wave = np.expand_dims(processed_wave, axis=-1)
                    spec_data = np.expand_dims(spec_data, axis=-1)

                    # è®¡ç®—ç‰¹å¾
                    fractal_dim = calculate_fractal_dimension(processed_wave[:, 0])
                    epicentral_distance = haversine_distance(
                        row["source_latitude_deg"], row["source_longitude_deg"],
                        row["station_latitude_deg"], row["station_longitude_deg"])
                    time_features = extract_time_features(row["origin_time"])

                    # ğŸ› ï¸ ä¿®å¤ï¼šæ„å»º8ç»´ç‰¹å¾å‘é‡ï¼ˆé¢„è®¡ç®—é˜¶æ®µï¼‰
                    feat_vector = np.array([
                        fractal_dim,
                        float(row["mag"]),
                        float(row["source_depth_km"]),
                        log_pg_sg,
                        epicentral_distance,
                        time_features[0],
                        time_features[1],
                        time_features[2]
                    ], dtype=np.float32)

                    # å½’ä¸€åŒ–å¤„ç†
                    if self.scaler is not None:
                        norm_part, dist, depth = split_feat_vector(feat_vector)
                        norm_part = self.scaler.transform(norm_part.reshape(1, -1)).flatten()
                        feat_vector = merge_feat_vector(norm_part, dist, depth)

                    #  å…³é”®ï¼šå¯¹æ¯æ¡æœ‰æ•ˆtraceè¿›è¡Œé¢„æµ‹
                    # å‡†å¤‡è¾“å…¥æ•°æ® - ä½¿ç”¨8ç»´ç‰¹å¾
                    wave_input = np.expand_dims(processed_wave, axis=0)  # (1, 4500, 1)
                    spec_input = np.expand_dims(spec_data, axis=0)  # (1, 75, 115, 1)
                    feat_input = np.expand_dims(feat_vector, axis=0)  # (1, 8) - 8ç»´ç‰¹å¾

                    # é¢„æµ‹æ¦‚ç‡
                    pred_prob = self.trace_model.predict([wave_input, spec_input, feat_input], verbose=0)
                    pred_prob = float(np.clip(pred_prob[0, 0], 0, 1))

                    probs[trace_name] = pred_prob
                    success_count += 1

                    del processed_wave, spec_data, raw_wave

                except Exception as e:
                    error_count += 1
                    if error_count <= 10:
                        print(f"é¢„è®¡ç®— trace {trace_name} å¤±è´¥: {str(e)}")
                    continue

        # è¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š
        coverage = success_count / total_traces * 100
        print(f"\n=== é¢„è®¡ç®—å®Œæˆç»Ÿè®¡ ===")
        print(f"æ€»traceæ•°: {total_traces}")
        print(f"æˆåŠŸé¢„æµ‹: {success_count} ({coverage:.2f}%)")
        print(f"å¤±è´¥: {error_count}")
        print(f"æ— æ•ˆæ³¢å½¢: {invalid_trace_count}")
        print(f"ç¼“å­˜è¦†ç›–ç‡: {len(probs)}/{total_traces} ({coverage:.2f}%)")

        if coverage < 98:
            print(f"âš ï¸  è­¦å‘Š: ç¼“å­˜è¦†ç›–ç‡è¾ƒä½ ({coverage:.2f}%)ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡")

        return probs

    def build_event_model(self):
        """æ„å»ºäº‹ä»¶çº§æ¨¡å‹ï¼šä½¿ç”¨ä¿®å¤çš„æ³¨æ„åŠ›å±‚"""

        # è¾“å…¥ç»´åº¦è°ƒæ•´ï¼šç‰¹å¾ä»9ç»´æ”¹ä¸º10ç»´
        event_wave_input = layers.Input(shape=(None, WAVEFORM_LENGTH, 1), name="event_waveform_input")
        event_spec_input = layers.Input(shape=(None, SPEC_HEIGHT, SPEC_WIDTH, 1), name="event_spectrogram_input")
        event_feat_input = layers.Input(shape=(None, 10), name="event_features_input")

        # Traceç¼–ç å™¨
        wave_enc, spec_enc, feat_enc = self._create_trace_encoder()

        # ç¼–ç æ¯ä¸ªtrace
        wave_emb = layers.TimeDistributed(wave_enc)(event_wave_input)
        spec_emb = layers.TimeDistributed(spec_enc)(event_spec_input)
        feat_emb = layers.TimeDistributed(feat_enc)(event_feat_input)

        # åˆå¹¶ç‰¹å¾
        combined = layers.Concatenate()([wave_emb, spec_emb, feat_emb])
        masked = layers.Masking(mask_value=0.0)(combined)

        # ğŸ› ï¸ ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨è´¨é‡æ„ŸçŸ¥æ³¨æ„åŠ›å±‚
        attention_weights = QualityAwareAttentionLayer(name="attention_weights")([masked, event_feat_input])

        # åŠ æƒç‰¹å¾èšåˆ
        weighted = layers.Multiply()([masked, attention_weights])
        event_emb = layers.GlobalAveragePooling1D()(weighted)

        # äº‹ä»¶çº§åˆ†ç±»è¾“å‡º
        event_out = layers.Dense(64, activation='relu')(event_emb)
        event_out = layers.Dropout(0.3)(event_out)
        event_out = layers.Dense(1, activation='sigmoid', name='event_output')(event_out)

        # Traceçº§è¾…åŠ©è¾“å‡º
        trace_out = layers.TimeDistributed(
            layers.Dense(1, activation='sigmoid'), name='trace_classifier')(masked)

        #  ä¿®å¤ï¼šåˆ›å»ºæ¨¡å‹æ—¶åªåŒ…å«éœ€è¦æŸå¤±å‡½æ•°çš„è¾“å‡º
        model = keras.Model(
            inputs=[event_wave_input, event_spec_input, event_feat_input],
            outputs=[event_out, trace_out]  # åªåŒ…å«ä¸¤ä¸ªéœ€è¦æŸå¤±å‡½æ•°çš„è¾“å‡º
        )

        #  ä¿®å¤ï¼šç¼–è¯‘é…ç½®åªåŒ…å«ä¸¤ä¸ªè¾“å‡º
        optimizer = build_safe_optimizer(LEARNING_RATE)
        model.compile(
            optimizer=optimizer,
            loss={
                'event_output': bulletproof_event_loss,
                'trace_classifier': bulletproof_trace_loss,
            },
            loss_weights={
                'event_output': 1.0,
                'trace_classifier': INTERMEDIATE_LOSS_WEIGHT,
            },
            metrics={
                'event_output': ['accuracy', 'precision', 'recall'],
                'trace_classifier': ['accuracy']
            }
        )

        #  æ–°å¢ï¼šåˆ›å»ºåŒ…å«æ³¨æ„åŠ›æƒé‡çš„å­æ¨¡å‹ç”¨äºå¯è§†åŒ–
        self.attention_model = keras.Model(
            inputs=[event_wave_input, event_spec_input, event_feat_input],
            outputs=attention_weights
        )

        self.model = model
        return model

    def _create_trace_encoder(self):
        """å…±äº« Trace ç¼–ç å™¨ï¼šç‰¹å¾åˆ†æ”¯å·²é€‚é… 10 ç»´è¾“å…¥"""
        # æ³¢å½¢åˆ†æ”¯
        wave_in = layers.Input(shape=(WAVEFORM_LENGTH, 1))
        x = layers.Conv1D(32, 5, activation='relu', padding='same')(wave_in)
        x = layers.MaxPooling1D(2)(x)
        x = layers.Conv1D(64, 5, activation='relu', padding='same')(x)
        x = layers.GlobalAveragePooling1D()(x)
        wave_enc = keras.Model(wave_in, x)

        # é¢‘è°±å›¾åˆ†æ”¯
        spec_in = layers.Input(shape=(SPEC_HEIGHT, SPEC_WIDTH, 1))
        y = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(spec_in)
        y = layers.MaxPooling2D((2, 2))(y)
        y = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(y)
        y = layers.GlobalAveragePooling2D()(y)
        spec_enc = keras.Model(spec_in, y)

        # ç‰¹å¾åˆ†æ”¯ï¼ˆ10 ç»´ï¼‰
        feat_in = layers.Input(shape=(10,))
        z = layers.Dense(32, activation='relu')(feat_in)
        z = layers.Dense(32, activation='relu')(z)
        feat_enc = keras.Model(feat_in, z)

        return wave_enc, spec_enc, feat_enc

    def quality_aware_attention(self, features, feat_inputs):
        """è´¨é‡æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶"""
        # ä»10ç»´ç‰¹å¾ä¸­æå–è´¨é‡åˆ†æ•°ï¼ˆç¬¬10ç»´ï¼‰
        batch_size = tf.shape(feat_inputs)[0]
        num_traces = tf.shape(feat_inputs)[1]
        quality_scores = feat_inputs[:, :, -1]  # æå–è´¨é‡åˆ†æ•°
        quality_scores = tf.reshape(quality_scores, (batch_size, num_traces, 1))

        # åŸºç¡€å†…å®¹æ³¨æ„åŠ›
        content_attention = layers.Dense(64, activation='relu')(features)
        content_attention = layers.Dense(1, activation='linear')(content_attention)

        # ç”¨è´¨é‡åˆ†æ•°è°ƒæ•´æ³¨æ„åŠ›
        # æ­£è´¨é‡å¢å¼ºæ³¨æ„åŠ›ï¼Œè´Ÿè´¨é‡æŠ‘åˆ¶æ³¨æ„åŠ›
        quality_adjustment = tf.where(
            quality_scores >= 0,
            tf.exp(quality_scores * 2),  # æ­£è´¨é‡ï¼šæŒ‡æ•°å¢å¼º
            tf.sigmoid(quality_scores * 10)  # è´Ÿè´¨é‡ï¼šsigmoidæŠ‘åˆ¶åˆ°æ¥è¿‘0
        )

        adjusted_attention = content_attention * quality_adjustment
        adjusted_attention = tf.squeeze(adjusted_attention, axis=-1)

        # softmaxå½’ä¸€åŒ–
        attention_weights = tf.nn.softmax(adjusted_attention, axis=1)

        return tf.expand_dims(attention_weights, -1)

    def validate_model_output_shapes(self):
        """éªŒè¯æ¨¡å‹è¾“å‡ºå½¢çŠ¶"""
        print("éªŒè¯æ¨¡å‹è¾“å‡ºå½¢çŠ¶:")
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
        dummy_batch_size = 2
        dummy_wave = tf.random.normal((dummy_batch_size, MAX_TRACES_PER_EVENT, WAVEFORM_LENGTH, 1))
        dummy_spec = tf.random.normal((dummy_batch_size, MAX_TRACES_PER_EVENT, SPEC_HEIGHT, SPEC_WIDTH, 1))
        dummy_feat = tf.random.normal((dummy_batch_size, MAX_TRACES_PER_EVENT, 8))

        # è·å–æ¨¡å‹è¾“å‡º
        outputs = self.model([dummy_wave, dummy_spec, dummy_feat])

        print(f"äº‹ä»¶çº§è¾“å‡ºå½¢çŠ¶: {outputs[0].shape}")
        print(f"Traceçº§è¾“å‡ºå½¢çŠ¶: {outputs[1].shape}")
        print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {outputs[2].shape}")  # æ–°å¢

        # åˆ›å»ºè™šæ‹Ÿæ ‡ç­¾
        dummy_event_labels = tf.constant([1, 0], dtype=tf.int8)  # å½¢çŠ¶ (2,)
        dummy_trace_labels = tf.constant([[1] * MAX_TRACES_PER_EVENT, [0] * MAX_TRACES_PER_EVENT],
                                         dtype=tf.int8)  # å½¢çŠ¶ (2, 10)

        # æµ‹è¯•æŸå¤±è®¡ç®—
        try:
            loss = self.model.test_on_batch(
                [dummy_wave, dummy_spec, dummy_feat],
                [dummy_event_labels, dummy_trace_labels]
            )
            print(f"æµ‹è¯•æŸå¤±è®¡ç®—æˆåŠŸ: {loss}")
        except Exception as e:
            print(f"æµ‹è¯•æŸå¤±è®¡ç®—å¤±è´¥: {e}")

    def pretrain_trace_model(self, train_metadata, val_metadata):
        """ä¿®å¤ç‰ˆï¼šé¢„è®­ç»ƒå•traceåˆ†ç±»æ¨¡å‹"""
        print("æ„å»ºå•traceè®­ç»ƒæ•°æ®é›†...")
        train_dataset = build_trace_tf_dataset(
            metadata=train_metadata,
            h5_manager=self.h5_manager,
            scaler=self.scaler,
            shuffle=True,
            batch_size=TRACE_BATCH_SIZE,
            is_training=True  # ğŸ› ï¸ ä¿®å¤ï¼šè®­ç»ƒé›†ä½¿ç”¨æ•°æ®å¢å¼º
        )

        print("æ„å»ºå•traceéªŒè¯æ•°æ®é›†...")
        val_dataset = build_trace_tf_dataset(
            metadata=val_metadata,
            h5_manager=self.h5_manager,
            scaler=self.scaler,
            shuffle=False,
            batch_size=TRACE_BATCH_SIZE,
            is_training=False  # ğŸ› ï¸ ä¿®å¤ï¼šéªŒè¯é›†ä¸ä½¿ç”¨æ•°æ®å¢å¼º
        )

        train_steps = max(1, len(train_metadata) // TRACE_BATCH_SIZE)
        val_steps = max(1, len(val_metadata) // TRACE_BATCH_SIZE)
        print(f"å•traceè®­ç»ƒæ­¥æ•°: {train_steps}, éªŒè¯æ­¥æ•°: {val_steps}")

        print("æ„å»ºå•traceæ¨¡å‹...")
        self.build_trace_model()

        #  å…³é”®ä¿®å¤ï¼šéªŒè¯æ¨¡å‹æ„å»ºæˆåŠŸ
        if self.trace_model is None:
            raise RuntimeError("å•Traceæ¨¡å‹æ„å»ºå¤±è´¥")

        self.trace_model.summary()

        y_train_labels = train_metadata["event_type"].apply(lambda x: 1 if x == "earthquake" else 0).values
        unique_classes = np.unique(y_train_labels)
        if len(unique_classes) < 2:
            print("è­¦å‘Š: è®­ç»ƒé›†ä¸­åªåŒ…å«ä¸€ç§äº‹ä»¶ç±»å‹ï¼Œè¿™ä¼šå½±å“å•traceæ¨¡å‹æ€§èƒ½")
            unique_classes = np.array([0, 1])
        base_class_weights = compute_class_weight("balanced", classes=unique_classes, y=y_train_labels)

        class_weight_dict = {}
        for i, cls in enumerate(unique_classes):
            if cls == 0:
                class_weight_dict[0] = base_class_weights[i] * EXPLOSION_WEIGHT_SCALE
            else:
                class_weight_dict[1] = base_class_weights[i]

        if 0 not in class_weight_dict:
            class_weight_dict[0] = 1.0
        if 1 not in class_weight_dict:
            class_weight_dict[1] = 1.0

        print(f"å•traceæ¨¡å‹ç±»åˆ«æƒé‡: {class_weight_dict}")

        explosion_recall_cb = ExplosionRecallLogger(val_dataset=val_dataset, val_steps=val_steps, is_trace_model=True)
        memory_cleaner_cb = MemoryCleaner()
        checkpoint_cb = keras.callbacks.ModelCheckpoint(
            os.path.join(CHECKPOINT_DIR, "trace_model_epoch_{epoch:02d}.keras"),
            save_freq='epoch',
            save_weights_only=False,
            verbose=1
        )

        # ä½¿ç”¨æ›´çµæ´»çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr_scheduler = ReduceLROnPlateau(
            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1
        )

        callbacks = [
            keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True, verbose=1, monitor="val_loss"),
            lr_scheduler,
            explosion_recall_cb,
            memory_cleaner_cb,
            checkpoint_cb
        ]

        print("å¼€å§‹å•traceæ¨¡å‹é¢„è®­ç»ƒ...")
        print_memory_usage("é¢„è®­ç»ƒå‰ ")

        with timing_context("å•traceæ¨¡å‹é¢„è®­ç»ƒ"):
            history = self.trace_model.fit(
                x=train_dataset,
                epochs=TRACE_PRETRAIN_EPOCHS,
                steps_per_epoch=train_steps,
                validation_data=val_dataset,
                validation_steps=val_steps,
                class_weight=class_weight_dict,
                callbacks=callbacks,
                verbose=1
            )

        print("å•traceæ¨¡å‹é¢„è®­ç»ƒå®Œæˆ")
        return history

    def fit_scaler(self, train_metadata):
        """ä»…å¯¹ 6 ç»´å¯å½’ä¸€åŒ–éƒ¨åˆ†æ‹Ÿåˆ scaler"""
        feat_list = []
        with self.h5_manager as h5_file:
            comp_order_cache = None
            for idx, (_, row) in enumerate(train_metadata.iterrows()):
                if idx % 1000 == 0:
                    print(f"Scalerè¿›åº¦: {idx}/{len(train_metadata)}")
                try:
                    if comp_order_cache is None:
                        comp_order_cache = get_component_order(h5_file)
                    target_comp_idx = comp_order_cache.index(VALID_COMPONENTS[0])
                    hdf5_path, event_idx = parse_trace_name(row["trace_name"])
                    raw_wave = h5_file[hdf5_path][event_idx, target_comp_idx, :].copy()
                    start_dt = parse_time_str(row["trace_start_time"])
                    raw_sr = float(row["trace_sampling_rate_hz"])
                    p_arrival = calculate_arrival_time(start_dt, int(row["trace_P_arrival_sample"]), raw_sr)
                    s_arrival = calculate_arrival_time(start_dt, int(row["trace_S_arrival_sample"]), raw_sr)
                    processed_wave, log_pg_sg = preprocess_waveform(raw_wave, raw_sr, p_arrival, s_arrival, start_dt)
                    fractal_dim = calculate_fractal_dimension(processed_wave)
                    epicentral_distance = haversine_distance(
                        row["source_latitude_deg"], row["source_longitude_deg"],
                        row["station_latitude_deg"], row["station_longitude_deg"])
                    time_features = extract_time_features(row["origin_time"])

                    # âœ… ä¸ split_feat_vector é¡ºåºå®Œå…¨ä¸€è‡´
                    feat_vector = np.array([
                        fractal_dim,  # 0
                        float(row["mag"]),  # 1
                        float(row["source_depth_km"]),  # 2
                        log_pg_sg,  # 3
                        epicentral_distance,  # 4
                        time_features[0],  # 5
                        time_features[1],  # 6
                        time_features[2]  # 7
                    ], dtype=np.float32)

                    norm_part, _, _ = split_feat_vector(feat_vector)
                    feat_list.append(norm_part)
                    del processed_wave, raw_wave
                except Exception:
                    continue

        if len(feat_list) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ç”¨äºscaler")
        self.scaler.fit(np.array(feat_list))

    def evaluate_test_set(self, test_name, test_metadata):
        """
        è¯„ä¼°å•ä¸ªæµ‹è¯•é›†ï¼šä¿®å¤è¾“å‡ºè§£åŒ…é—®é¢˜
        """
        try:
            # è·å–äº‹ä»¶åˆ†ç»„ç»Ÿè®¡ä¿¡æ¯
            test_event_groups = group_metadata_by_event(test_metadata)

            # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            total_events = len(test_event_groups)
            earthquake_events = sum(1 for event_type, _ in test_event_groups if event_type == "earthquake")
            explosion_events = sum(1 for event_type, _ in test_event_groups if event_type == "explosion")

            # å•traceäº‹ä»¶è¯¦ç»†ç»Ÿè®¡
            single_trace_events = sum(1 for _, traces in test_event_groups if len(traces) == 1)
            multi_trace_events = total_events - single_trace_events

            print(f"{test_name} åŒ…å« {total_events} ä¸ªäº‹ä»¶")

            # é¢„è®¡ç®— trace æ¦‚ç‡
            all_traces = pd.concat([tr for _, tr in test_event_groups], ignore_index=True)
            if self.trace_prob_cache is None:
                self.trace_prob_cache = self.precompute_trace_probs(all_traces, self.h5_manager)
            trace_prob_cache = self.trace_prob_cache

            test_dataset = build_event_tf_dataset(
                event_groups=test_event_groups,
                h5_path=WAVEFORM_PATH,
                scaler=self.scaler,
                shuffle=False,
                batch_size=BATCH_SIZE,
                trace_prob_cache=trace_prob_cache,
                is_training=False  # ğŸ› ï¸ ä¿®å¤ï¼šæµ‹è¯•é›†ä¸ä½¿ç”¨æ•°æ®å¢å¼º
            )

            test_steps = max(1, len(test_event_groups) // BATCH_SIZE)

            # ------- æ”¶é›†é¢„æµ‹ & æ ‡ç­¾ -------
            y_true, y_pred_prob = [], []
            trace_pred_flat, trace_true_flat = [], []

            #  ä¿®å¤ï¼šç°åœ¨åªæœ‰ä¸¤ä¸ªè¾“å‡º
            for (x1, x2, x3), (y_event, y_trace) in test_dataset.take(test_steps):
                # é¢„æµ‹
                preds = self.model.predict([x1, x2, x3], verbose=0)
                event_pred = preds[0].flatten()
                trace_pred = preds[1]

                # äº‹ä»¶çº§
                batch_y_event = np.asarray(y_event).ravel().tolist()
                y_true.extend(batch_y_event)
                y_pred_prob.extend(event_pred.tolist())

                # trace çº§ï¼šå»æ‰ padding
                batch_size = y_event.shape[0]
                for b in range(batch_size):
                    n_real = tf.reduce_sum(tf.cast(y_trace[b] != -1, tf.int32)).numpy()
                    if n_real == 0:
                        continue
                    trace_pred_flat.extend(trace_pred[b, :n_real, 0].tolist())
                    trace_true_flat.extend(y_trace[b, :n_real].numpy().tolist())

            # ------- è®¡ç®—è¯¦ç»†æŒ‡æ ‡ -------
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

            # äº‹ä»¶çº§æŒ‡æ ‡
            flat_y_true = [int(item) for item in y_true]
            y_pred = (np.array(y_pred_prob) > 0.5).astype(int)

            event_accuracy = accuracy_score(flat_y_true, y_pred)
            event_precision = precision_score(flat_y_true, y_pred, zero_division=0)
            event_recall = recall_score(flat_y_true, y_pred, zero_division=0)
            event_f1 = f1_score(flat_y_true, y_pred, zero_division=0)

            # äº‹ä»¶çº§æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(flat_y_true, y_pred)
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
                explosion_recall = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                earthquake_recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                explosion_precision = tn / (tn + fn) if (tn + fn) > 0 else 0.0
                earthquake_precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            else:
                explosion_recall = earthquake_recall = explosion_precision = earthquake_precision = 0.0
                tn, fp, fn, tp = 0, 0, 0, 0

            # Traceçº§æŒ‡æ ‡
            if len(trace_pred_flat) > 0:
                trace_pred_binary = (np.array(trace_pred_flat) > 0.5).astype(int)
                trace_accuracy = accuracy_score(trace_true_flat, trace_pred_binary)
                trace_precision = precision_score(trace_true_flat, trace_pred_binary, zero_division=0)
                trace_recall = recall_score(trace_true_flat, trace_pred_binary, zero_division=0)
                trace_f1 = f1_score(trace_true_flat, trace_pred_binary, zero_division=0)

                # Traceçº§æ··æ·†çŸ©é˜µ
                trace_cm = confusion_matrix(trace_true_flat, trace_pred_binary)
                if trace_cm.shape == (2, 2):
                    t_tn, t_fp, t_fn, t_tp = trace_cm.ravel()
                    trace_explosion_recall = t_tn / (t_tn + t_fp) if (t_tn + t_fp) > 0 else 0.0
                    trace_earthquake_recall = t_tp / (t_tp + t_fn) if (t_tp + t_fn) > 0 else 0.0
                else:
                    trace_explosion_recall = trace_earthquake_recall = 0.0
                    t_tn, t_fp, t_fn, t_tp = 0, 0, 0, 0
            else:
                trace_accuracy = trace_precision = trace_recall = trace_f1 = 0.0
                trace_explosion_recall = trace_earthquake_recall = 0.0
                t_tn, t_fp, t_fn, t_tp = 0, 0, 0, 0

            # ------- è¾“å‡ºè¯¦ç»†ç»“æœ -------
            output_lines = [
                f"\n[{test_name} - è¯¦ç»†æµ‹è¯•ç»“æœ]",
                "=" * 80,
                "\n[æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯]",
                f"- åŸå§‹äº‹ä»¶æ€»æ•°: {len(test_metadata['event_id'].unique())}",
                f"- æœ€ç»ˆäº‹ä»¶æ•° (åŒ…å«å•trace): {total_events}",
                f"- äº‹ä»¶åˆ©ç”¨ç‡: {total_events / len(test_metadata['event_id'].unique()) * 100:.1f}%",
                f"- åœ°éœ‡äº‹ä»¶: {earthquake_events}, çˆ†ç‚¸äº‹ä»¶: {explosion_events}",
                f"- å•traceäº‹ä»¶: {single_trace_events} ({single_trace_events / total_events * 100:.1f}%)",
                f"- å¤štraceäº‹ä»¶: {multi_trace_events} ({multi_trace_events / total_events * 100:.1f}%)",
                "\n[äº‹ä»¶çº§æ€§èƒ½æŒ‡æ ‡]",
                f"- å‡†ç¡®ç‡ (Accuracy): {event_accuracy:.4f}",
                f"- ç²¾ç¡®ç‡ (Precision): {event_precision:.4f}",
                f"- å¬å›ç‡ (Recall): {event_recall:.4f}",
                f"- F1åˆ†æ•°: {event_f1:.4f}",
                f"- åœ°éœ‡å¬å›ç‡: {earthquake_recall:.4f}",
                f"- çˆ†ç‚¸å¬å›ç‡: {explosion_recall:.4f}",
                f"- åœ°éœ‡ç²¾ç¡®ç‡: {earthquake_precision:.4f}",
                f"- çˆ†ç‚¸ç²¾ç¡®ç‡: {explosion_precision:.4f}",
                f"- æ··æ·†çŸ©é˜µ: TN={tn}, FP={fp}, FN={fn}, TP={tp}",
                "\n[Traceçº§æ€§èƒ½æŒ‡æ ‡]",
                f"- å‡†ç¡®ç‡ (Accuracy): {trace_accuracy:.4f}",
                f"- ç²¾ç¡®ç‡ (Precision): {trace_precision:.4f}",
                f"- å¬å›ç‡ (Recall): {trace_recall:.4f}",
                f"- F1åˆ†æ•°: {trace_f1:.4f}",
                f"- åœ°éœ‡å¬å›ç‡: {trace_earthquake_recall:.4f}",
                f"- çˆ†ç‚¸å¬å›ç‡: {trace_explosion_recall:.4f}",
                f"- æ€»Traceæ•°: {len(trace_pred_flat)}",
                f"- æ··æ·†çŸ©é˜µ: TN={t_tn}, FP={t_fp}, FN={t_fn}, TP={t_tp}",
                "\n" + "=" * 80 + "\n"
            ]

            out_str = "\n".join(output_lines)
            print(out_str)
            with open(RESULT_OUTPUT_PATH, "a", encoding="utf-8") as f:
                f.write(out_str)

            # ------- ç”»å›¾ -------
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

            # äº‹ä»¶çº§æ··æ·†çŸ©é˜µ
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=['Explosion', 'Earthquake'],
                        yticklabels=['Explosion', 'Earthquake'], ax=ax1)
            ax1.set_title('Event-Level Confusion Matrix')

            # Traceçº§æ··æ·†çŸ©é˜µ
            if len(trace_pred_flat) > 0:
                sns.heatmap(trace_cm, annot=True, fmt='d', cmap='Oranges',
                            xticklabels=['Explosion', 'Earthquake'],
                            yticklabels=['Explosion', 'Earthquake'], ax=ax2)
            ax2.set_title('Trace-Level Confusion Matrix')

            plt.tight_layout()
            both_cm_path = CONFUSION_MATRIX_PATH.replace('.png', f'_{test_name.replace(" ", "_")}_both.png')
            plt.savefig(both_cm_path, dpi=200, bbox_inches='tight')
            plt.close()

        except Exception as e:
            import traceback
            err_msg = f"åŠ è½½ {test_name} å¤±è´¥: {str(e)}\n" + traceback.format_exc()
            error_logger.error(err_msg)
            print(err_msg)
            with open(RESULT_OUTPUT_PATH, "a", encoding="utf-8") as f:
                f.write(f"é”™è¯¯: {err_msg}\n\n" + "=" * 80 + "\n\n")

    # ------------------------------------------------------------------
    # æ–°å¢ï¼šä¿®å¤è´Ÿ loss çš„å›è°ƒï¼ˆæ”¾åœ¨ train æ–¹æ³•å¤–é¢ä¹Ÿè¡Œï¼Œè¿™é‡Œç›´æ¥å†…åµŒï¼‰
    # ------------------------------------------------------------------
    class FixTotalLossCallback(Callback):
        """
        æŠŠ logs['loss'] ä¿®æ­£ä¸º
            1.0 * event_output_loss + INTERMEDIATE_LOSS_WEIGHT * trace_classifier_loss
        è¦†ç›– TensorFlow æ—¥å¿—é‡Œçš„è´Ÿå€¼ bugã€‚
        """

        def on_epoch_end(self, epoch, logs=None):
            if logs is None:
                return
            ev_loss = logs.get('event_output_loss')
            tr_loss = logs.get('trace_classifier_loss')
            if ev_loss is not None and tr_loss is not None:
                logs['loss'] = ev_loss + INTERMEDIATE_LOSS_WEIGHT * tr_loss

    def train(self, train_path, val_path, test_sets, waveform_path, skip_training=False):
        """ä¿®å¤ç‰ˆï¼šç¡®ä¿æ¨¡å‹æ­£ç¡®æ„å»ºå’Œè®­ç»ƒ"""
        self.h5_manager = H5FileManager(waveform_path)
        trace_history = None
        event_history = None

        # ============== å†…åµŒä¿®å¤ç‰ˆå›è°ƒ ===============
        class FixedTracePerformanceLogger(Callback):
            def __init__(self, val_dataset, val_steps):
                super().__init__()
                self.val_dataset = val_dataset
                self.val_steps = val_steps

            def on_epoch_end(self, epoch, logs=None):
                y_true, y_pred_prob = [], []
                try:
                    for (x1, x2, x3), (y_event, y_trace) in self.val_dataset.take(self.val_steps):
                        preds = self.model.predict([x1, x2, x3], verbose=0)
                        trace_pred = preds[1] if isinstance(preds, list) and len(preds) >= 2 else preds

                        batch_size = y_event.shape[0]
                        for b in range(batch_size):
                            real_mask = y_trace[b] != -1
                            n_real = tf.reduce_sum(tf.cast(real_mask, tf.int32)).numpy()
                            if n_real == 0:
                                continue
                            pred_real = trace_pred[b, :n_real, 0] if len(trace_pred.shape) == 3 else trace_pred[
                                b, :n_real]
                            y_true.extend(y_trace[b, :n_real].numpy().tolist())
                            y_pred_prob.extend(pred_real.numpy().flatten().tolist())

                    if not y_true:
                        logs.update({'val_trace_accuracy': 0.0, 'val_trace_precision': 0.0, 'val_trace_recall': 0.0})
                        return

                    y_pred = (np.array(y_pred_prob) > 0.5).astype(int)
                    acc = np.mean(np.array(y_true) == y_pred)
                    cm = confusion_matrix(y_true, y_pred)
                    if cm.shape == (2, 2):
                        tn, fp, fn, tp = cm.ravel()
                        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                    else:
                        prec = rec = 0.0

                    logs.update({'val_trace_accuracy': acc, 'val_trace_precision': prec, 'val_trace_recall': rec})
                    print(f" - Validation trace accuracy: {acc:.4f}, precision: {prec:.4f}, recall: {rec:.4f}")

                except Exception as e:
                    print(f"Traceæ€§èƒ½è®°å½•å™¨é”™è¯¯: {e}")
                    logs.update({'val_trace_accuracy': 0.0, 'val_trace_precision': 0.0, 'val_trace_recall': 0.0})

        try:
            print("æ£€æŸ¥HDF5æ–‡ä»¶å®Œæ•´æ€§...")
            check_h5_integrity(waveform_path)

            print("åŠ è½½è®­ç»ƒå…ƒæ•°æ®...")
            train_metadata = load_metadata_from_split(train_path)
            val_metadata = load_metadata_from_split(val_path)
            train_event_groups = group_metadata_by_event(train_metadata)
            val_event_groups = group_metadata_by_event(val_metadata)

            if not skip_training:
                print("æ‹Ÿåˆç‰¹å¾æ ‡å‡†åŒ–å™¨...")
                self.fit_scaler(train_metadata)

                # ğŸ› ï¸ å…³é”®ä¿®å¤1ï¼šå…ˆé¢„è®­ç»ƒå•Traceæ¨¡å‹
                print("å¼€å§‹é¢„è®­ç»ƒå•Traceæ¨¡å‹...")
                trace_history = self.pretrain_trace_model(train_metadata, val_metadata)

                # ğŸ› ï¸ å…³é”®ä¿®å¤2ï¼šæ„å»ºäº‹ä»¶çº§æ¨¡å‹
                print("æ„å»ºäº‹ä»¶çº§æ¨¡å‹...")
                self.build_event_model()

                # -------------- é¢„è®¡ç®— trace æ¦‚ç‡ç¼“å­˜ --------------
                print("é¢„è®¡ç®—è®­ç»ƒé›†traceæ¦‚ç‡...")
                train_trace_prob_cache = self.precompute_trace_probs(train_metadata, self.h5_manager)
                print("é¢„è®¡ç®—éªŒè¯é›†traceæ¦‚ç‡...")
                val_trace_prob_cache = self.precompute_trace_probs(val_metadata, self.h5_manager)

                # -------------- æ„å»ºæ•°æ®é›†ï¼ˆå¸¦ repeatï¼‰ --------------
                print("æ„å»ºè®­ç»ƒæ•°æ®é›†...")
                train_dataset = build_event_tf_dataset(
                    train_event_groups, waveform_path, self.scaler, shuffle=True,
                    batch_size=BATCH_SIZE, trace_prob_cache=train_trace_prob_cache, is_training=True
                ).repeat()  # ğŸ› ï¸ é˜²è€—å°½

                print("æ„å»ºéªŒè¯æ•°æ®é›†...")
                val_dataset = build_event_tf_dataset(
                    val_event_groups, waveform_path, self.scaler, shuffle=False,
                    batch_size=BATCH_SIZE, trace_prob_cache=val_trace_prob_cache, is_training=False
                )

                # -------------- æ­£ç¡®è®¡ç®— stepsï¼ˆå‘ä¸Šå–æ•´ï¼‰ --------------
                train_steps = int(np.ceil(len(train_event_groups) / BATCH_SIZE))
                val_steps = int(np.ceil(len(val_event_groups) / BATCH_SIZE))
                print(f"è®­ç»ƒæ­¥æ•°: {train_steps} | éªŒè¯æ­¥æ•°: {val_steps}")

                #  å…³é”®ä¿®å¤3ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦æ„å»ºæˆåŠŸ
                if self.model is None:
                    raise RuntimeError("äº‹ä»¶çº§æ¨¡å‹æ„å»ºå¤±è´¥ï¼Œæ— æ³•è®­ç»ƒ")

                print("äº‹ä»¶çº§æ¨¡å‹æ„å»ºæˆåŠŸï¼Œå¼€å§‹è®­ç»ƒ...")
                self.model.summary()

                # -------------- å›è°ƒ --------------
                lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)
                explosion_recall_cb = ExplosionRecallLogger(val_dataset=val_dataset, val_steps=val_steps)
                memory_cleaner_cb = MemoryCleaner()
                checkpoint_cb = keras.callbacks.ModelCheckpoint(
                    os.path.join(CHECKPOINT_DIR, "event_model_epoch_{epoch:02d}.keras"),
                    save_freq='epoch', save_weights_only=False, verbose=1
                )

                callbacks = [
                    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1,
                                                  monitor="val_loss"),
                    lr_scheduler,
                    explosion_recall_cb,
                    FixedTracePerformanceLogger(val_dataset=val_dataset, val_steps=val_steps),  # ğŸ› ï¸ ä¿®å¤ç‰ˆ
                    memory_cleaner_cb,
                    checkpoint_cb,
                    self.FixTotalLossCallback(),
                ]

                # -------------- è®­ç»ƒ --------------
                print("å¼€å§‹äº‹ä»¶çº§æ¨¡å‹è®­ç»ƒ...")
                print_memory_usage("è®­ç»ƒå‰ ")

                with timing_context("äº‹ä»¶çº§æ¨¡å‹è®­ç»ƒ"):
                    event_history = self.model.fit(
                        train_dataset,
                        epochs=EPOCHS,
                        steps_per_epoch=train_steps,
                        validation_data=val_dataset,
                        validation_steps=val_steps,
                        callbacks=callbacks,
                        verbose=1
                    )

                self.save_model(SAVE_MODEL_PATH)
                print("æ¨¡å‹å·²ä¿å­˜è‡³:", SAVE_MODEL_PATH)
            else:
                print("è·³è¿‡è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
                self.load_model(SAVE_MODEL_PATH)

            # -------------- æµ‹è¯•è¯„ä¼° --------------
            for test_name, test_path in test_sets:
                print(f"è¯„ä¼°æµ‹è¯•é›†: {test_name}")
                test_metadata = load_metadata_from_split(test_path)
                self.evaluate_test_set(test_name, test_metadata)

            # -------------- ç»Ÿä¸€ç¼“å­˜ --------------
            print("æ„å»ºç»Ÿä¸€traceæ¦‚ç‡ç¼“å­˜...")
            all_traces = pd.concat(
                [pd.concat([tr for _, tr in train_event_groups], ignore_index=True),
                 pd.concat([tr for _, tr in val_event_groups], ignore_index=True)] +
                [pd.concat([tr for _, tr in group_metadata_by_event(load_metadata_from_split(tp))]) for _, tp in
                 test_sets],
                ignore_index=True
            )
            self.trace_prob_cache = self.precompute_trace_probs(all_traces, self.h5_manager)

            # -------------- ç»˜å›¾ --------------
            if not skip_training and event_history:
                self.plot_optimized_training_history(trace_history, event_history)
            self.plot_trace_performance(val_event_groups)
            self.plot_quality_vs_attention(val_event_groups)
            self.plot_trace_model_quality(val_event_groups)

            return (trace_history, event_history)

        except Exception as e:
            error_logger.error("è®­ç»ƒä¸­æ–­: %s", e, exc_info=True)
            raise e
        finally:
            self.h5_manager.close()
            print("è®­ç»ƒæµç¨‹ç»“æŸï¼ˆHDF5 å·²å…³é—­ï¼‰")

    def plot_quality_vs_attention(self, val_event_groups, test_event_groups=None, save_suffix='quality'):
        """
        å®Œæ•´æ›¿æ¢ç‰ˆï¼šä¿®å¤è¾“å‡ºè§£åŒ…é—®é¢˜ & å¤ç”¨ trace_prob_cache
        """
        import matplotlib.pyplot as plt
        from statsmodels.nonparametric.smoothers_lowess import lowess

        # ---------- 1. æ•°æ®æ”¶é›† ----------
        all_groups = val_event_groups + (test_event_groups or [])
        if not all_groups:
            print("è­¦å‘Šï¼šæ— äº‹ä»¶æ•°æ®å¯å¤„ç†")
            return

        all_traces = pd.concat([tr for _, tr in all_groups], ignore_index=True)

        # âœ… å¤ç”¨ç¼“å­˜ï¼Œä¸å†é‡å¤é¢„è®¡ç®—
        if self.trace_prob_cache is None:
            raise RuntimeError("trace_prob_cache æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œ train() æˆ–æ‰‹åŠ¨è®¾ç½® cache")
        trace_prob_cache = self.trace_prob_cache

        dataset = build_event_tf_dataset(
            all_groups, WAVEFORM_PATH, self.scaler, False, 32, trace_prob_cache
        )
        steps = min(100, max(1, len(all_groups) // 32))

        distances, qualities, attention_weights = [], [], []
        event_types, depths, correctness_labels = [], [], []

        #  ä¿®å¤ï¼šç°åœ¨åªæœ‰ä¸¤ä¸ªè¾“å‡º (y_event, y_trace)
        for (x1, x2, x3), (y_event, y_trace) in dataset.take(steps):
            preds = self.model.predict([x1, x2, x3], verbose=0)

            #  ä¿®å¤ï¼šä½¿ç”¨æ³¨æ„åŠ›å­æ¨¡å‹è·å–æ³¨æ„åŠ›æƒé‡
            attn_weights = self.attention_model.predict([x1, x2, x3], verbose=0)
            trace_pred = preds[1]

            batch_size = y_event.shape[0]
            for b in range(batch_size):
                feats = x3.numpy()[b]
                labels = y_trace.numpy()[b]
                ev_type = 'Earthquake' if y_event.numpy()[b] == 1 else 'Explosion'
                n_real = tf.reduce_sum(tf.cast(labels != -1, tf.int32)).numpy()
                if n_real == 0:
                    continue
                for i in range(n_real):
                    attn = attn_weights[b, i] if attn_weights.ndim == 2 else attn_weights[b, i, 0]
                    if attn <= 1e-8:
                        continue
                    pred_lab = 1 if trace_pred[b, i, 0] > 0.5 else 0
                    true_lab = int(labels[i])
                    correct = (pred_lab == true_lab)
                    prob = np.clip(trace_pred[b, i, 0], 1e-5, 1 - 1e-5)
                    logit = np.log(prob / (1 - prob))
                    confidence = abs(np.clip(logit, -10, 10))
                    quality = confidence if correct else -confidence
                    distances.append(feats[i, 4])
                    depths.append(feats[i, 2])
                    qualities.append(quality)
                    attention_weights.append(attn)
                    event_types.append(ev_type)
                    correctness_labels.append(correct)

        qualities = np.array(qualities)
        correctness_labels = np.array(correctness_labels)
        distances = np.array(distances)
        depths = np.array(depths)
        event_types = np.array(event_types)

        n_correct = int(correctness_labels.sum())
        n_total = len(correctness_labels)
        n_error = n_total - n_correct  # âœ… ç®€å•å‡æ³•
        acc = n_correct / n_total

        print(f'\n===== Trace æ¨¡å‹å››å­å›¾ Quality è¯Šæ–­æ­£ç¡®ç‡ç»Ÿè®¡ =====')
        print(f'æ€»æ ·æœ¬æ•°: {n_total}')
        print(f'æ­£ç¡®æ ·æœ¬: {n_correct} | é”™è¯¯æ ·æœ¬: {n_error}')
        print(f'æ•´ä½“æ­£ç¡®ç‡: {acc:.4f}')
        print(f'Quality ä¸ºè´Ÿçš„æ¯”ä¾‹: {(qualities < 0).mean():.2%}')
        print('=============================================\n')

        # ---------- 3. ç»˜å›¾ ----------
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Event Model: Signed Quality Diagnostics', fontsize=16)

        # å›¾1 è·ç¦» vs signed quality
        colors1 = np.where(correctness_labels, 'blue', 'red')
        ax1.scatter(distances, qualities, c=colors1, s=20, alpha=0.7, edgecolors='k', linewidths=0.5)
        if distances.size > 10:
            order = np.argsort(distances)
            trend = lowess(qualities[order], distances[order], frac=0.3, return_sorted=False)
            ax1.plot(distances[order], trend, color='black', lw=2, label='LOWESS (all)')
        ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)
        ax1.set_xlabel('Epicentral Distance (km)')
        ax1.set_ylabel('Signed Quality (wrong<0, right>0)')
        ax1.set_title('Distance vs Signed Quality')
        ax1.legend();
        ax1.grid(True, alpha=0.3)

        # å›¾2 ç›´æ–¹å›¾ï¼ˆå¸¦è´Ÿè½´ï¼‰
        max_abs = max(abs(qualities)) if qualities.size else 1
        bins = np.linspace(-max_abs, max_abs, 41)
        ax2.hist(qualities, bins=bins, color='steelblue', alpha=0.7, edgecolor='k')
        ax2.axvline(0, color='k', linestyle='--')
        ax2.set_xlabel('Signed Quality')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Signed Quality Distribution')
        ax2.grid(True, alpha=0.3)

        # ---------- 3. ç¬¬3å­å›¾ï¼šéœ‡ä¸­è· vs Signed Qualityï¼ˆå®Œæ•´å›¾ä¾‹ï¼‰ ----------
        # æ•£ç‚¹ï¼šåœ°éœ‡=é’è‰²ï¼Œçˆ†ç ´=çº¢è‰²
        eq_mask = event_types == 'Earthquake'
        ex_mask = ~eq_mask
        ax3.scatter(distances[eq_mask], qualities[eq_mask],
                    c='cyan', s=20, alpha=0.7, edgecolors='k', linewidths=0.5,
                    label='Earthquake')
        ax3.scatter(distances[ex_mask], qualities[ex_mask],
                    c='red', s=20, alpha=0.7, edgecolors='k', linewidths=0.5,
                    label='Explosion')

        # è¶‹åŠ¿çº¿ï¼šåœ°éœ‡=æ·±ç»¿ï¼Œçˆ†ç ´=æ·±æ©™
        for et, col_trend, msk in [('Earthquake', 'darkgreen', eq_mask),
                                   ('Explosion', 'darkorange', ex_mask)]:
            if msk.sum() < 10:
                continue
            x_m = np.array(distances)[msk]
            y_m = np.array(qualities)[msk]
            order = np.argsort(x_m)
            trend = lowess(y_m[order], x_m[order], frac=0.3, return_sorted=False)
            ax3.plot(x_m[order], trend, color=col_trend, lw=2.5,
                     label=f'{et} trend')

        ax3.axhline(0, color='gray', linestyle='--', alpha=0.5)
        ax3.set_xlabel('Epicentral Distance (km)')
        ax3.set_ylabel('Signed Quality')
        ax3.set_title('Distance vs Signed Quality')
        ax3.legend()  # â† ç°åœ¨å››é¡¹å…¨éƒ¨å‡ºç°
        ax3.grid(True, alpha=0.3)

        # å›¾4 æ·±åº¦ vs Signed Quality
        ax4.scatter(depths, qualities, c=colors1, s=20, alpha=0.7, edgecolors='k', linewidths=0.5)
        if depths.size > 10:
            order = np.argsort(depths)
            trend = lowess(qualities[order], depths[order], frac=0.3, return_sorted=False)
            ax4.plot(depths[order], trend, color='black', lw=2, label='LOWESS (all)')
        ax4.axhline(0, color='gray', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Source Depth (km)')
        ax4.set_ylabel('Signed Quality (wrong<0, right>0)')
        ax4.set_title('Depth vs Signed Quality')
        ax4.legend();
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = TRACE_ATTENTION_HEATMAP_PATH.replace(
            '.png', f'_{save_suffix}_signed_quality_complete.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f'å·²ä¿å­˜å®Œæ•´æ›¿æ¢ç‰ˆ quality å›¾ï¼š{save_path}')

    def plot_trace_model_quality(self, val_event_groups, test_event_groups=None, save_suffix='trace_model_quality'):
        """
        ç»˜åˆ¶å•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­å›¾
        çºµåæ ‡ï¼šå•TRACEæ¨¡å‹å¯¹TRACEçš„åˆ¤åˆ«è´¨é‡ (2*y_true-1)*(2*y_pred-1)
        """
        import matplotlib.pyplot as plt
        from statsmodels.nonparametric.smoothers_lowess import lowess

        # ---------- 1. æ•°æ®æ”¶é›† ----------
        all_groups = val_event_groups + (test_event_groups or [])
        if not all_groups:
            print("è­¦å‘Šï¼šæ— äº‹ä»¶æ•°æ®å¯å¤„ç†")
            return

        all_traces = pd.concat([tr for _, tr in all_groups], ignore_index=True)

        # âœ… å¤ç”¨ç¼“å­˜ï¼Œä¸å†é‡å¤é¢„è®¡ç®—
        if self.trace_prob_cache is None:
            raise RuntimeError("trace_prob_cache æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè¿è¡Œ train() æˆ–æ‰‹åŠ¨è®¾ç½® cache")
        trace_prob_cache = self.trace_prob_cache

        dataset = build_event_tf_dataset(
            all_groups, WAVEFORM_PATH, self.scaler, False, 32, trace_prob_cache
        )
        steps = min(100, max(1, len(all_groups) // 32))

        distances, trace_qualities, attention_weights = [], [], []
        event_types, depths, correctness_labels = [], [], []

        #  ä¿®å¤ï¼šç°åœ¨åªæœ‰ä¸¤ä¸ªè¾“å‡º (y_event, y_trace)
        for (x1, x2, x3), (y_event, y_trace) in dataset.take(steps):
            # ä½¿ç”¨å•TRACEæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡è®¡ç®—è´¨é‡
            batch_size = y_event.shape[0]
            for b in range(batch_size):
                feats = x3.numpy()[b]
                labels = y_trace.numpy()[b]
                ev_type = 'Earthquake' if y_event.numpy()[b] == 1 else 'Explosion'
                n_real = tf.reduce_sum(tf.cast(labels != -1, tf.int32)).numpy()
                if n_real == 0:
                    continue

                for i in range(n_real):
                    # ä»ç‰¹å¾ä¸­æå–å•TRACEæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ï¼ˆç¬¬9ç»´ï¼Œç´¢å¼•8ï¼‰
                    trace_prob = feats[i, 8]  # ç¬¬9ç»´æ˜¯traceæ¦‚ç‡
                    true_label = int(labels[i])

                    # ğŸ› ï¸ ä½¿ç”¨ä¸å•TRACEæ¨¡å‹ç›¸åŒçš„è´¨é‡å‡½æ•°
                    trace_quality = float((2 * true_label - 1) * (2 * trace_prob - 1))

                    # è®¡ç®—æ­£ç¡®æ€§
                    pred_label = 1 if trace_prob > 0.5 else 0
                    correct = (pred_label == true_label)

                    distances.append(feats[i, 4])  # éœ‡ä¸­è·
                    depths.append(feats[i, 2])  # æ·±åº¦
                    trace_qualities.append(trace_quality)
                    event_types.append(ev_type)
                    correctness_labels.append(correct)

        trace_qualities = np.array(trace_qualities)
        correctness_labels = np.array(correctness_labels)
        distances = np.array(distances)
        depths = np.array(depths)
        event_types = np.array(event_types)

        n_correct = int(correctness_labels.sum())
        n_total = len(correctness_labels)
        n_error = n_total - n_correct
        acc = n_correct / n_total

        print(f'\n===== å•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­ç»Ÿè®¡ =====')
        print(f'æ€»æ ·æœ¬æ•°: {n_total}')
        print(f'æ­£ç¡®æ ·æœ¬: {n_correct} | é”™è¯¯æ ·æœ¬: {n_error}')
        print(f'æ•´ä½“æ­£ç¡®ç‡: {acc:.4f}')
        print(f'è´¨é‡ä¸ºæ­£çš„æ¯”ä¾‹: {(trace_qualities > 0).mean():.2%}')
        print(f'è´¨é‡å¹³å‡å€¼: {trace_qualities.mean():.4f}')
        print(f'è´¨é‡æ ‡å‡†å·®: {trace_qualities.std():.4f}')
        print('==================================\n')

        # ---------- 2. ç»˜å›¾ ----------
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Trace Model Quality Diagnostics (Quality = (2*y_true-1)*(2*y_pred-1))', fontsize=16)

        # å›¾1 è·ç¦» vs å•TRACEæ¨¡å‹è´¨é‡
        colors1 = np.where(correctness_labels, 'blue', 'red')
        ax1.scatter(distances, trace_qualities, c=colors1, s=20, alpha=0.7, edgecolors='k', linewidths=0.5)
        if distances.size > 10:
            order = np.argsort(distances)
            trend = lowess(trace_qualities[order], distances[order], frac=0.3, return_sorted=False)
            ax1.plot(distances[order], trend, color='black', lw=2, label='LOWESS (all)')
        ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)
        ax1.set_xlabel('Epicentral Distance (km)')
        ax1.set_ylabel('Trace Model Quality')
        ax1.set_title('Distance vs Trace Model Quality')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å›¾2 ç›´æ–¹å›¾ï¼ˆå¸¦è´Ÿè½´ï¼‰
        max_abs = max(abs(trace_qualities)) if trace_qualities.size else 1
        bins = np.linspace(-max_abs, max_abs, 41)
        ax2.hist(trace_qualities, bins=bins, color='steelblue', alpha=0.7, edgecolor='k')
        ax2.axvline(0, color='k', linestyle='--')
        ax2.set_xlabel('Trace Model Quality')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Trace Model Quality Distribution')
        ax2.grid(True, alpha=0.3)

        # å›¾3 è·ç¦» vs å•TRACEæ¨¡å‹è´¨é‡ï¼ˆæŒ‰äº‹ä»¶ç±»å‹åˆ†ç±»ï¼‰
        eq_mask = event_types == 'Earthquake'
        ex_mask = ~eq_mask
        ax3.scatter(distances[eq_mask], trace_qualities[eq_mask],
                    c='cyan', s=20, alpha=0.7, edgecolors='k', linewidths=0.5,
                    label='Earthquake')
        ax3.scatter(distances[ex_mask], trace_qualities[ex_mask],
                    c='red', s=20, alpha=0.7, edgecolors='k', linewidths=0.5,
                    label='Explosion')

        # è¶‹åŠ¿çº¿ï¼šåœ°éœ‡=æ·±ç»¿ï¼Œçˆ†ç ´=æ·±æ©™
        for et, col_trend, msk in [('Earthquake', 'darkgreen', eq_mask),
                                   ('Explosion', 'darkorange', ex_mask)]:
            if msk.sum() < 10:
                continue
            x_m = np.array(distances)[msk]
            y_m = np.array(trace_qualities)[msk]
            order = np.argsort(x_m)
            trend = lowess(y_m[order], x_m[order], frac=0.3, return_sorted=False)
            ax3.plot(x_m[order], trend, color=col_trend, lw=2.5,
                     label=f'{et} trend')

        ax3.axhline(0, color='gray', linestyle='--', alpha=0.5)
        ax3.set_xlabel('Epicentral Distance (km)')
        ax3.set_ylabel('Trace Model Quality')
        ax3.set_title('Distance vs Trace Model Quality (by Event Type)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # å›¾4 æ·±åº¦ vs å•TRACEæ¨¡å‹è´¨é‡
        ax4.scatter(depths, trace_qualities, c=colors1, s=20, alpha=0.7, edgecolors='k', linewidths=0.5)
        if depths.size > 10:
            order = np.argsort(depths)
            trend = lowess(trace_qualities[order], depths[order], frac=0.3, return_sorted=False)
            ax4.plot(depths[order], trend, color='black', lw=2, label='LOWESS (all)')
        ax4.axhline(0, color='gray', linestyle='--', alpha=0.5)
        ax4.set_xlabel('Source Depth (km)')
        ax4.set_ylabel('Trace Model Quality')
        ax4.set_title('Depth vs Trace Model Quality')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = TRACE_ATTENTION_HEATMAP_PATH.replace(
            '.png', f'_{save_suffix}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f'å·²ä¿å­˜å•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­å›¾ï¼š{save_path}')

        # ---------- 3. é¢å¤–åˆ†æï¼šè´¨é‡ä¸æ­£ç¡®ç‡çš„å…³ç³» ----------
        plt.figure(figsize=(12, 8))

        # æŒ‰è´¨é‡åˆ†ç®±è®¡ç®—æ­£ç¡®ç‡
        quality_bins = np.linspace(-1, 1, 21)
        bin_centers = (quality_bins[:-1] + quality_bins[1:]) / 2
        bin_accuracies = []
        bin_counts = []

        for i in range(len(quality_bins) - 1):
            mask = (trace_qualities >= quality_bins[i]) & (trace_qualities < quality_bins[i + 1])
            if mask.sum() > 0:
                bin_acc = correctness_labels[mask].mean()
                bin_accuracies.append(bin_acc)
                bin_counts.append(mask.sum())
            else:
                bin_accuracies.append(0)
                bin_counts.append(0)

        # ç»˜åˆ¶è´¨é‡-æ­£ç¡®ç‡å…³ç³»å›¾
        plt.subplot(2, 2, 1)
        plt.plot(bin_centers, bin_accuracies, 'o-', linewidth=2, markersize=6)
        plt.xlabel('Trace Model Quality')
        plt.ylabel('Accuracy')
        plt.title('Quality vs Accuracy')
        plt.grid(True, alpha=0.3)

        # ç»˜åˆ¶æ ·æœ¬æ•°é‡åˆ†å¸ƒ
        plt.subplot(2, 2, 2)
        plt.bar(bin_centers, bin_counts, width=0.08, alpha=0.7)
        plt.xlabel('Trace Model Quality')
        plt.ylabel('Sample Count')
        plt.title('Quality Distribution')
        plt.grid(True, alpha=0.3)

        # ç»˜åˆ¶è´¨é‡ä¸è·ç¦»çš„å…³ç³»ï¼ˆæŒ‰æ­£ç¡®æ€§ï¼‰
        plt.subplot(2, 2, 3)
        correct_mask = correctness_labels
        plt.scatter(distances[correct_mask], trace_qualities[correct_mask],
                    c='green', s=15, alpha=0.6, label='Correct')
        plt.scatter(distances[~correct_mask], trace_qualities[~correct_mask],
                    c='red', s=15, alpha=0.6, label='Incorrect')
        plt.xlabel('Epicentral Distance (km)')
        plt.ylabel('Trace Model Quality')
        plt.title('Distance vs Quality (by Correctness)')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # ç»˜åˆ¶è´¨é‡ä¸æ·±åº¦çš„å…³ç³»ï¼ˆæŒ‰æ­£ç¡®æ€§ï¼‰
        plt.subplot(2, 2, 4)
        plt.scatter(depths[correct_mask], trace_qualities[correct_mask],
                    c='green', s=15, alpha=0.6, label='Correct')
        plt.scatter(depths[~correct_mask], trace_qualities[~correct_mask],
                    c='red', s=15, alpha=0.6, label='Incorrect')
        plt.xlabel('Source Depth (km)')
        plt.ylabel('Trace Model Quality')
        plt.title('Depth vs Quality (by Correctness)')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        analysis_path = TRACE_ATTENTION_HEATMAP_PATH.replace(
            '.png', f'_{save_suffix}_analysis.png')
        plt.savefig(analysis_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f'å·²ä¿å­˜å•TRACEæ¨¡å‹è´¨é‡åˆ†æå›¾ï¼š{analysis_path}')

    def plot_trace_training_history(self, trace_history):
        """ç»˜åˆ¶å•Traceæ¨¡å‹è®­ç»ƒå†å²"""
        if trace_history is None:
            print("æ²¡æœ‰å•Traceè®­ç»ƒå†å²æ•°æ®")
            return

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()

        # å®šä¹‰è¦ç»˜åˆ¶çš„æŒ‡æ ‡
        metrics = [
            ('accuracy', 'Accuracy'),
            ('loss', 'Loss'),
            ('precision', 'Precision'),
            ('recall', 'Recall')
        ]

        for i, (metric_key, metric_name) in enumerate(metrics):
            ax = axes[i]

            # è®­ç»ƒé›†æŒ‡æ ‡
            train_metric = trace_history.history.get(metric_key)
            if train_metric is not None:
                ax.plot(train_metric, label=f'Training {metric_name}',
                        linewidth=2, color='#1f77b4')

            # éªŒè¯é›†æŒ‡æ ‡
            val_metric_key = f'val_{metric_key}'
            val_metric = trace_history.history.get(val_metric_key)
            if val_metric is not None:
                ax.plot(val_metric, label=f'Validation {metric_name}',
                        linewidth=2, color='#ff7f0e', linestyle='--')

            ax.set_title(f'Trace Model - {metric_name}', fontsize=14, fontweight='bold')
            ax.set_xlabel('Epoch')
            ax.set_ylabel(metric_name)
            ax.legend()
            ax.grid(True, alpha=0.3)

            # å¦‚æœæ˜¯å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ï¼Œè®¾ç½®yè½´èŒƒå›´ä¸º[0,1]
            if metric_key in ['accuracy', 'precision', 'recall']:
                ax.set_ylim(0, 1.05)

        plt.tight_layout()
        plt.savefig(TRACE_HISTORY_PLOT_PATH, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"å•Traceæ¨¡å‹è®­ç»ƒå†å²å›¾å·²ä¿å­˜è‡³: {TRACE_HISTORY_PLOT_PATH}")

    def plot_event_training_history(self, event_history):
        """event è®­ç»ƒå†å²å›¾ï¼šåªä¿ç•™å’Œ trace å®Œå…¨ä¸€è‡´çš„ 4 ä¸ªæŒ‡æ ‡"""
        if event_history is None:
            print("æ²¡æœ‰äº‹ä»¶çº§è®­ç»ƒå†å²æ•°æ®")
            return

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()
        keys = ['accuracy', 'loss', 'precision', 'recall']
        titles = ['Accuracy', 'Loss', 'Precision', 'Recall']

        for ax, k, t in zip(axes, keys, titles):
            tr_key = f'event_output_{k}'
            val_tr_key = f'val_{tr_key}'

            # è®­ç»ƒé›†
            if tr_key in event_history.history:
                ax.plot(event_history.history[tr_key],
                        label=f'Training {t}', linewidth=2, color='#1f77b4')

            # éªŒè¯é›†
            if val_tr_key in event_history.history:
                ax.plot(event_history.history[val_tr_key],
                        label=f'Validation {t}', linewidth=2, color='#ff7f0e', linestyle='--')

            ax.set_title(f'Event Model - {t}', fontsize=14, fontweight='bold')
            ax.set_xlabel('Epoch')
            ax.set_ylabel(t)
            ax.legend()
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(EVENT_HISTORY_PLOT_PATH, dpi=200, bbox_inches='tight')
        plt.close()
        print(f"äº‹ä»¶çº§æ¨¡å‹è®­ç»ƒå†å²å›¾å·²ä¿å­˜è‡³: {EVENT_HISTORY_PLOT_PATH}")

    def plot_optimized_training_history(self, trace_history, event_history):
        """ä¼˜åŒ–çš„è®­ç»ƒå†å²å›¾ - åˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„å›¾"""
        # ç»˜åˆ¶å•Traceæ¨¡å‹è®­ç»ƒå†å²
        self.plot_trace_training_history(trace_history)

        # ç»˜åˆ¶äº‹ä»¶çº§æ¨¡å‹è®­ç»ƒå†å²
        self.plot_event_training_history(event_history)

    def plot_trace_performance(self, val_event_groups):
        """ä¿®å¤ç‰ˆï¼šç¡®ä¿æ•°æ®èƒ½å¤Ÿæ­£ç¡®æ”¶é›†"""
        if len(val_event_groups) == 0:
            print("è­¦å‘Š: éªŒè¯äº‹ä»¶ç»„ä¸ºç©º")
            return

        # é¢„è®¡ç®— trace æ¦‚ç‡
        if self.trace_prob_cache is None:
            print("è­¦å‘Š: trace_prob_cache ä¸ºç©ºï¼Œæ— æ³•æ„å»ºæ•°æ®é›†")
            return

        trace_prob_cache = self.trace_prob_cache

        # æ„å»ºéªŒè¯æ•°æ®é›†
        try:
            print(f"æ„å»ºéªŒè¯æ•°æ®é›†ï¼Œäº‹ä»¶æ•°: {len(val_event_groups)}")
            val_dataset = build_event_tf_dataset(
                val_event_groups,
                WAVEFORM_PATH,
                self.scaler,
                shuffle=False,
                batch_size=BATCH_SIZE,
                trace_prob_cache=trace_prob_cache
            )

            # è®¡ç®—åˆé€‚çš„éªŒè¯æ­¥æ•°
            val_steps = min(50, max(1, len(val_event_groups) // BATCH_SIZE))
            print(f"éªŒè¯æ•°æ®é›†: {len(val_event_groups)} äº‹ä»¶, {val_steps} æ­¥")

            if val_steps == 0:
                print("é”™è¯¯: éªŒè¯æ­¥æ•°ä¸º0ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
                return

        except Exception as e:
            print(f"æ„å»ºéªŒè¯æ•°æ®é›†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return

        event_true, event_pred_prob = [], []
        trace_pred_flat, trace_true_flat = [], []

        try:
            # ğŸ› ï¸ å…³é”®ä¿®å¤ï¼šç›´æ¥è¿­ä»£æ•°æ®é›†ï¼Œä¸ä½¿ç”¨ take()
            print("å¼€å§‹æ•°æ®æ”¶é›†...")
            batch_count = 0
            total_samples = 0

            for batch in val_dataset:
                if batch_count >= val_steps:  # æ‰‹åŠ¨æ§åˆ¶æ‰¹æ¬¡æ•°é‡
                    break

                inputs, outputs = batch
                x1, x2, x3 = inputs
                y_event, y_trace = outputs

                #  ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ•°æ®æœ‰æ•ˆ
                if (x1.shape[0] == 0 or x2.shape[0] == 0 or x3.shape[0] == 0):
                    print(f"æ‰¹æ¬¡ {batch_count}: è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                #  ä¿®å¤ï¼šæ¨¡å‹ç°åœ¨è¿”å›ä¸¤ä¸ªè¾“å‡º
                try:
                    preds = self.model.predict([x1, x2, x3], verbose=0, steps=1)
                except Exception as e:
                    print(f"æ‰¹æ¬¡ {batch_count}: é¢„æµ‹å¤±è´¥: {e}")
                    continue

                # äº‹ä»¶çº§é¢„æµ‹æ˜¯ç¬¬ä¸€ä¸ªè¾“å‡º
                if isinstance(preds, list) and len(preds) >= 1:
                    event_pred = preds[0]
                else:
                    event_pred = preds

                event_pred_prob.extend(event_pred.flatten().tolist())
                event_true.extend(y_event.numpy().tolist())

                # traceçº§é¢„æµ‹æ˜¯ç¬¬äºŒä¸ªè¾“å‡º
                trace_pred = None
                if isinstance(preds, list) and len(preds) >= 2:
                    trace_pred = preds[1]
                elif hasattr(preds, 'shape') and len(preds.shape) > 2:
                    trace_pred = preds

                batch_size = y_event.shape[0]
                for b in range(batch_size):
                    # è®¡ç®—çœŸå®traceæ•°é‡ï¼ˆæ’é™¤å¡«å……çš„-1ï¼‰
                    try:
                        real_mask = y_trace[b] != -1
                        n_real = tf.reduce_sum(tf.cast(real_mask, tf.int32)).numpy()

                        if n_real == 0:
                            continue

                        #  ä¿®å¤ï¼šæ­£ç¡®å¤„ç†traceé¢„æµ‹å½¢çŠ¶
                        if trace_pred is not None:
                            # trace_pred å½¢çŠ¶åº”è¯¥æ˜¯ (batch_size, max_traces, 1)
                            if len(trace_pred.shape) == 3:
                                batch_trace_pred = trace_pred[b, :n_real, 0]  # å–å‰n_realä¸ªï¼Œå»æ‰æœ€åä¸€ä¸ªç»´åº¦
                            elif len(trace_pred.shape) == 2:
                                batch_trace_pred = trace_pred[b, :n_real]  # å½¢çŠ¶å¯èƒ½æ˜¯ (batch_size, max_traces)
                            else:
                                print(f"æ‰¹æ¬¡ {batch_count}: æœªçŸ¥çš„traceé¢„æµ‹å½¢çŠ¶: {trace_pred.shape}")
                                continue

                            trace_pred_flat.extend(batch_trace_pred.flatten().tolist())

                        # çœŸå®æ ‡ç­¾
                        trace_true_flat.extend(y_trace[b, :n_real].numpy().tolist())
                        total_samples += n_real

                    except Exception as e:
                        print(f"æ‰¹æ¬¡ {batch_count} æ ·æœ¬ {b} å¤„ç†å¤±è´¥: {e}")
                        continue

                batch_count += 1
                if batch_count % 10 == 0:
                    print(f"å¤„ç†äº† {batch_count} æ‰¹æ¬¡, ç´¯è®¡ {total_samples} ä¸ªtraceæ ·æœ¬")

            print(f"æ•°æ®æ”¶é›†å®Œæˆ: {len(event_true)} äº‹ä»¶, {len(trace_true_flat)} traces, {batch_count} æ‰¹æ¬¡")

        except Exception as e:
            print(f"æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if len(event_true) == 0:
            print("é”™è¯¯: æ²¡æœ‰æ”¶é›†åˆ°äº‹ä»¶çº§æ•°æ®")
            print("è°ƒè¯•ä¿¡æ¯:")
            print(f"- æ‰¹æ¬¡å¤„ç†æ•°: {batch_count}")
            print(f"- éªŒè¯æ­¥æ•°: {val_steps}")
            print(f"- äº‹ä»¶ç»„æ•°é‡: {len(val_event_groups)}")
            return

        if len(trace_true_flat) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æ”¶é›†åˆ°traceçº§æ•°æ®ï¼Œåªç»˜åˆ¶äº‹ä»¶çº§æŒ‡æ ‡")

        from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

        # äº‹ä»¶çº§æŒ‡æ ‡è®¡ç®—
        event_pred = [int(p > 0.5) for p in event_pred_prob]
        acc_e = accuracy_score(event_true, event_pred)
        pre_e = precision_score(event_true, event_pred, zero_division=0)
        rec_e = recall_score(event_true, event_pred, zero_division=0)
        f1_e = f1_score(event_true, event_pred, zero_division=0)

        print(f"äº‹ä»¶çº§æŒ‡æ ‡ - å‡†ç¡®ç‡: {acc_e:.4f}, ç²¾ç¡®ç‡: {pre_e:.4f}, å¬å›ç‡: {rec_e:.4f}, F1: {f1_e:.4f}")

        # Traceçº§æŒ‡æ ‡è®¡ç®—
        if len(trace_true_flat) > 0:
            trace_pred_binary = (np.array(trace_pred_flat) > 0.5).astype(int)
            acc_t = accuracy_score(trace_true_flat, trace_pred_binary)
            pre_t = precision_score(trace_true_flat, trace_pred_binary, zero_division=0)
            rec_t = recall_score(trace_true_flat, trace_pred_binary, zero_division=0)
            f1_t = f1_score(trace_true_flat, trace_pred_binary, zero_division=0)

            print(f"Traceçº§æŒ‡æ ‡ - å‡†ç¡®ç‡: {acc_t:.4f}, ç²¾ç¡®ç‡: {pre_t:.4f}, å¬å›ç‡: {rec_t:.4f}, F1: {f1_t:.4f}")
        else:
            acc_t = pre_t = rec_t = f1_t = 0.0
            print("Traceçº§æŒ‡æ ‡: æ— æ•°æ®")

        # ç»˜åˆ¶æŒ‡æ ‡å¯¹æ¯”å›¾
        metrics_name = ['Accuracy', 'Precision', 'Recall', 'F1-score']
        event_vals = [acc_e, pre_e, rec_e, f1_e]
        trace_vals = [acc_t, pre_t, rec_t, f1_t]

        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(1, 4, figsize=(20, 5))
        colors = ['#1f77b4', '#ff7f0e']

        for i, (ax, m_name, e_val, t_val) in enumerate(zip(axes, metrics_name, event_vals, trace_vals)):
            labels = ['Event-Level', 'Trace-Level']
            values = [e_val, t_val]

            bars = ax.bar(labels, values, color=colors, alpha=0.8, edgecolor='k')
            ax.set_ylim(0, 1.05)
            ax.set_ylabel('Score')
            ax.set_title(f'{m_name}')

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, v in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                        f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

            ax.grid(axis='y', alpha=0.3)
            ax.set_xticklabels(labels, rotation=45)

        plt.suptitle('Model Performance: Event-Level vs Trace-Level', fontsize=16, fontweight='bold')
        plt.tight_layout()

        # ä¿å­˜åˆå¹¶çš„æŒ‡æ ‡å›¾
        merged_path = TRACE_PERFORMANCE_PATH.replace('.png', '_merged.png')
        plt.savefig(merged_path, dpi=200, bbox_inches='tight', facecolor='white')
        plt.show()
        print(f'æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜ï¼š{merged_path}')

        # ç»˜åˆ¶æ¦‚ç‡åˆ†å¸ƒå›¾
        plt.figure(figsize=(10, 6))

        if len(event_pred_prob) > 0:
            event_probs = np.array(event_pred_prob)
            plt.hist(event_probs, bins=30, alpha=0.6, label='Event-Level',
                     color='#1f77b4', density=True)
            print(f"äº‹ä»¶çº§æ¦‚ç‡åˆ†å¸ƒ: å‡å€¼={event_probs.mean():.4f}, æ ‡å‡†å·®={event_probs.std():.4f}")

        if len(trace_pred_flat) > 0:
            trace_probs = np.array(trace_pred_flat)
            plt.hist(trace_probs, bins=30, alpha=0.6, label='Trace-Level',
                     color='#ff7f0e', density=True)
            print(f"Traceçº§æ¦‚ç‡åˆ†å¸ƒ: å‡å€¼={trace_probs.mean():.4f}, æ ‡å‡†å·®={trace_probs.std():.4f}")

        plt.xlabel('Prediction Probability')
        plt.ylabel('Density')
        plt.title('Prediction Probability Distribution')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.tight_layout()

        dist_path = TRACE_PERFORMANCE_PATH.replace('.png', '_prob_dist.png')
        plt.savefig(dist_path, dpi=200, bbox_inches='tight', facecolor='white')
        plt.show()
        print(f'æ¦‚ç‡åˆ†å¸ƒå›¾å·²ä¿å­˜ï¼š{dist_path}')

    def _build_attention_model(self):
        if self.model is None:
            raise RuntimeError("å¿…é¡»å…ˆåŠ è½½ self.model æ‰èƒ½æ„å»º attention_model")
        # æå–è¾“å…¥
        inputs = self.model.input  # [wave, spec, feat]
        # æå–æ³¨æ„åŠ›å±‚è¾“å‡º
        attention_layer = self.model.get_layer("attention_weights")
        attention_output = attention_layer.output
        # æ„å»ºå­æ¨¡å‹
        self.attention_model = keras.Model(inputs=inputs, outputs=attention_output)

    def save_model(self, path):
        os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
        if not path.endswith('.keras'):
            path += '.keras'

        self.model.save(path)
        trace_model_path = path.replace(".keras", "_trace_model.keras")
        if hasattr(self, 'trace_model') and self.trace_model is not None:
            self.trace_model.save(trace_model_path)

        scaler_path = path.replace(".keras", "_scaler.joblib")
        joblib.dump(self.scaler, scaler_path)

        # âœ… ä¿å­˜ç¼“å­˜
        if self.trace_prob_cache is not None:
            cache_path = path.replace(".keras", "_trace_probs.joblib")
            joblib.dump(self.trace_prob_cache, cache_path)
            print(f"trace æ¦‚ç‡ç¼“å­˜å·²ä¿å­˜åˆ°: {cache_path}")

    def load_model(self, path):
        import keras
        self.model = keras.models.load_model(path, custom_objects=CUSTOM_OBJECTS)
        self._build_attention_model()

        trace_model_path = path.replace(".keras", "_trace_model.keras")
        if os.path.exists(trace_model_path):
            self.trace_model = keras.models.load_model(trace_model_path, custom_objects=CUSTOM_OBJECTS)

        scaler_path = path.replace(".keras", "_scaler.joblib")
        self.scaler = joblib.load(scaler_path)

        # âœ… åŠ è½½ç¼“å­˜
        cache_path = path.replace(".keras", "_trace_probs.joblib")
        if os.path.exists(cache_path):
            self.trace_prob_cache = joblib.load(cache_path)
            print(f"trace æ¦‚ç‡ç¼“å­˜å·²åŠ è½½: {cache_path}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ° trace æ¦‚ç‡ç¼“å­˜ï¼Œç”»å›¾æ—¶å°†é‡æ–°è®¡ç®—ï¼ˆå»ºè®®å…ˆ train()ï¼‰")

    def load_latest_checkpoint(self):
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        if not os.path.exists(CHECKPOINT_DIR):
            return None

        checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('event_model_') and f.endswith('.keras')]
        if not checkpoints:
            return None

        checkpoints.sort(key=lambda x: os.path.getmtime(os.path.join(CHECKPOINT_DIR, x)), reverse=True)
        latest_checkpoint = os.path.join(CHECKPOINT_DIR, checkpoints[0])
        print(f"åŠ è½½æœ€æ–°äº‹ä»¶çº§æ¨¡å‹æ£€æŸ¥ç‚¹: {latest_checkpoint}")

        self.model = keras.models.load_model(
            latest_checkpoint,
            custom_objects=CUSTOM_OBJECTS
        )

        trace_checkpoint = latest_checkpoint.replace('event_model_', 'trace_model_')
        if os.path.exists(trace_checkpoint):
            self.trace_model = keras.models.load_model(trace_checkpoint)

        scaler_path = latest_checkpoint.replace(".keras", "_scaler.joblib")
        if os.path.exists(scaler_path):
            self.scaler = joblib.load(scaler_path)

        return latest_checkpoint


# -------------------------- ä¸»å‡½æ•° --------------------------
def main():
    try:
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"ä½¿ç”¨GPU: {[gpu.name for gpu in gpus]}")
        else:
            print("ä½¿ç”¨CPU")
    except RuntimeError as e:
        print(f"GPUé…ç½®é”™è¯¯: {e}")

    model_exists = os.path.exists(SAVE_MODEL_PATH)
    trace_model_path = SAVE_MODEL_PATH.replace(".keras", "_trace_model.keras")
    trace_model_exists = os.path.exists(trace_model_path)
    scaler_path = SAVE_MODEL_PATH.replace(".keras", "_scaler.joblib")
    scaler_exists = os.path.exists(scaler_path)

    skip_training = model_exists and trace_model_exists and scaler_exists

    if skip_training:
        print(f"å·²æ‰¾åˆ°å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹: {SAVE_MODEL_PATH}")
        print("å°†è·³è¿‡è®­ç»ƒï¼Œç›´æ¥è¿›è¡Œæµ‹è¯•å’Œå¯è§†åŒ–")
    else:
        print("æœªæ‰¾åˆ°å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹")
        print("å°†å¼€å§‹è®­ç»ƒè¿‡ç¨‹")

    classifier = EarthquakeClassifier(COLUMN_MAPPING)

    try:
        print("=" * 60)
        if skip_training:
            print("æ”¹è¿›çš„åœ°éœ‡ä¸çˆ†ç‚¸åˆ†ç±» - ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        else:
            print("æ”¹è¿›çš„åœ°éœ‡ä¸çˆ†ç‚¸åˆ†ç±»æ¨¡å‹è®­ç»ƒå¼€å§‹")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

        if skip_training:
            print(f"ä» {SAVE_MODEL_PATH} åŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
            try:
                classifier.load_model(SAVE_MODEL_PATH)
                classifier.h5_manager = H5FileManager(WAVEFORM_PATH)
                print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
            except Exception as e:
                print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                print("å°†é‡æ–°è®­ç»ƒæ¨¡å‹...")
                skip_training = False

        if not skip_training:
            classifier.h5_manager = H5FileManager(WAVEFORM_PATH)

        # ========== è®­ç»ƒ/æµ‹è¯• ==========
        print("å¼€å§‹è®­ç»ƒæµç¨‹...")
        hist = classifier.train(
            train_path=TRAIN_PATH,
            val_path=VAL_PATH,
            test_sets=TEST_SETS,
            waveform_path=WAVEFORM_PATH,
            skip_training=skip_training
        )

        #  ä¿®å¤ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸæ„å»º
        if classifier.model is None:
            print("é”™è¯¯: æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ¨¡å‹ä¸ºNone!")
            return

        # ========== éªŒè¯ä¿å­˜-åŠ è½½ ==========
        print("ä¿å­˜æ¨¡å‹...")
        classifier.save_model(SAVE_MODEL_PATH)

        # éªŒè¯æ¨¡å‹å¯ä»¥é‡æ–°åŠ è½½
        print("éªŒè¯æ¨¡å‹é‡æ–°åŠ è½½...")
        try:
            loaded_model = keras.models.load_model(SAVE_MODEL_PATH, custom_objects=CUSTOM_OBJECTS)
            print("âœ… æ¨¡å‹å·²å¯æ­£å¸¸é‡æ–°åŠ è½½ï¼")
        except Exception as e:
            print(f"âŒ æ¨¡å‹é‡æ–°åŠ è½½å¤±è´¥: {e}")

        # ========== å‡†å¤‡äº‹ä»¶ç»„ ==========
        print("å‡†å¤‡éªŒè¯å’Œæµ‹è¯•äº‹ä»¶ç»„...")
        val_metadata = load_metadata_from_split(VAL_PATH)
        val_event_groups = group_metadata_by_event(val_metadata)

        test_event_groups = []
        for _, test_path in TEST_SETS:
            test_meta = load_metadata_from_split(test_path)
            test_event_groups.extend(group_metadata_by_event(test_meta))

        # ========== ç»Ÿä¸€ç»˜å›¾ ==========
        print('\n====== å¼€å§‹ç»˜åˆ¶æ€§èƒ½ä¸å¯è§†åŒ–å›¾è¡¨ ======')

        # 1. è®­ç»ƒå†å²
        if hist is not None:
            trace_history, event_history = hist
            if trace_history is not None and event_history is not None:
                classifier.plot_optimized_training_history(trace_history, event_history)
                print("è®­ç»ƒå†å²å›¾å·²ä¿å­˜")
            else:
                print('ï¼ˆè·³è¿‡è®­ç»ƒå†å²å›¾ï¼šæ— å†å²æ•°æ®ï¼‰')
        else:
            print('ï¼ˆè·³è¿‡è®­ç»ƒå†å²å›¾ï¼šæ— è®­ç»ƒå†å²ï¼‰')

        # 2. éªŒè¯é›†æ€§èƒ½
        print("ç»˜åˆ¶Traceæ€§èƒ½å¯¹æ¯”å›¾...")
        try:
            classifier.plot_trace_performance(val_event_groups)
            print("Traceæ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"ç»˜åˆ¶Traceæ€§èƒ½å¯¹æ¯”å›¾å¤±è´¥: {e}")

        # 3. æ³¨æ„åŠ›æƒé‡ç›¸å…³
        print("ç»˜åˆ¶äº‹ä»¶æ¨¡å‹è´¨é‡è¯Šæ–­å›¾...")
        try:
            classifier.plot_quality_vs_attention(val_event_groups, test_event_groups)
            print("äº‹ä»¶æ¨¡å‹è´¨é‡è¯Šæ–­å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"ç»˜åˆ¶äº‹ä»¶æ¨¡å‹è´¨é‡è¯Šæ–­å›¾å¤±è´¥: {e}")

        # 4. æ–°å¢ï¼šå•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­å›¾
        print("ç»˜åˆ¶å•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­å›¾...")
        try:
            classifier.plot_trace_model_quality(val_event_groups, test_event_groups)
            print("å•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­å›¾å·²ä¿å­˜")
        except Exception as e:
            print(f"ç»˜åˆ¶å•TRACEæ¨¡å‹è´¨é‡è¯Šæ–­å›¾å¤±è´¥: {e}")

        print('====== æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°é¡¹ç›®ç›®å½• ======')

        # ========== æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ ==========
        print("\n" + "=" * 60)
        print("æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
        print(f"- éªŒè¯é›†äº‹ä»¶æ•°: {len(val_event_groups)}")
        print(
            f"- æµ‹è¯•é›†äº‹ä»¶æ•°: {sum(len(group_metadata_by_event(load_metadata_from_split(test_path))) for _, test_path in TEST_SETS)}")

        # è®¡ç®—æ€»ä½“traceæ•°é‡
        total_traces = 0
        for _, traces in val_event_groups:
            total_traces += len(traces)
        for test_name, test_path in TEST_SETS:
            test_meta = load_metadata_from_split(test_path)
            test_groups = group_metadata_by_event(test_meta)
            for _, traces in test_groups:
                total_traces += len(traces)

        print(f"- æ€»traceæ•°: {total_traces}")
        print(f"- æ¨¡å‹ä¿å­˜è·¯å¾„: {SAVE_MODEL_PATH}")
        print(f"- ç»“æœè¾“å‡ºè·¯å¾„: {RESULT_OUTPUT_PATH}")
        print("=" * 60)

    except Exception as e:
        import traceback
        print("\n" + "=" * 60)
        print("å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:")
        traceback.print_exc()
        print("=" * 60)

        #  ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å¤‡ä»½æ–‡ä»¶å
        if hasattr(classifier, 'model') and classifier.model is not None:
            print("å°è¯•ä¿å­˜å½“å‰æ¨¡å‹...")
            try:
                backup_path = SAVE_MODEL_PATH.replace('.keras', '_backup.keras')
                classifier.save_model(backup_path)
                print(f"æ¨¡å‹å·²å¤‡ä»½ä¿å­˜åˆ°: {backup_path}")
            except Exception as save_error:
                print(f"æ¨¡å‹å¤‡ä»½å¤±è´¥: {save_error}")
        exit(1)


if __name__ == "__main__":
    main()
